{"cells":[{"cell_type":"markdown","metadata":{"id":"A4pHAHnoyGVA"},"source":["## Instructions\n","\n","You are asked to complete the following files:\n","* **pruned_layers.py**, which contains the pruning of DNNs to reduce the storage of insignificant weight parameters by two methods: pruning by percentage and prune by standara deviation.\n","* **train_util.py**, which includes the training process of DNNs with pruned connections.\n","* **quantize.py**, which applies the quantization (weight sharing) part on the DNN to reduce the storage of weight parameters.\n","* **huffman_coding.py**, which applies the Huffman coding onto the weight of DNNs to further compress the weight size.\n","\n","You are asked to submit the following files:\n","* **net_before_pruning.pt**, which is the weight parameters before applying pruning on DNN weight parameters.\n","* **net_after_pruning.pt**, which is the weight paramters after applying pruning on DNN weight parameters.\n","* **net_after_quantization.pt**, which is the weight parameters after applying quantization (weight sharing) on DNN weight parameters.\n","* **codebook_vgg16.npy**, which is the quantization codebook of each layer after applying quantization (weight sharing).\n","* **huffman_encoding.npy**, which is the encoding map of each item within the quantization codebook in the whole DNN architecture.\n","* **huffman_freq.npy**, which is the frequency map of each item within the quantization codebook in the whole DNN.\n","\n","To ensure fair grading policy, we fix the choice of model to VGG16_half, which is a down-scaled version of VGG16 using a width multiplier of 0.5. You may check the implementation in **vgg16.py** for more details."]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount ('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L3sJKqss9fNG","executionInfo":{"status":"ok","timestamp":1698443149643,"user_tz":240,"elapsed":1330,"user":{"displayName":"Amarnath Shinde","userId":"15881715752542230515"}},"outputId":"117175aa-0d52-440b-a293-c490ea4b09af"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["cd '/content/drive/MyDrive/CSC591-791-Lab2/'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oi8sXZay9l-c","executionInfo":{"status":"ok","timestamp":1698443151528,"user_tz":240,"elapsed":314,"user":{"displayName":"Amarnath Shinde","userId":"15881715752542230515"}},"outputId":"ce5c551f-1d75-424a-9df9-6f8c85d691b3"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/CSC591-791-Lab2\n"]}]},{"cell_type":"code","execution_count":6,"metadata":{"pycharm":{"is_executing":false},"id":"HcwRYzV_yGVD","executionInfo":{"status":"ok","timestamp":1698443153392,"user_tz":240,"elapsed":486,"user":{"displayName":"Amarnath Shinde","userId":"15881715752542230515"}}},"outputs":[],"source":["from vgg16 import VGG16, VGG16_half\n","from train_util import train, finetune_after_prune, test\n","from quantize import quantize_whole_model\n","from huffman_coding import huffman_coding\n","from summary import summary\n","import torch\n","import numpy as np\n","from prune import prune\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'"]},{"cell_type":"markdown","metadata":{"id":"hWPZxPJnyGVE"},"source":["### Full-precision model training"]},{"cell_type":"code","execution_count":7,"metadata":{"pycharm":{"is_executing":false,"name":"#%%\n"},"colab":{"base_uri":"https://localhost:8080/"},"id":"DBbCcf_UyGVE","executionInfo":{"status":"ok","timestamp":1698443162716,"user_tz":240,"elapsed":6734,"user":{"displayName":"Amarnath Shinde","userId":"15881715752542230515"}},"outputId":"4fc42123-354a-4dfe-dfec-58a243655bb5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":7}],"source":["net = VGG16_half()\n","net = net.to(device)\n","\n","# Uncomment to load pretrained weights\n","net.load_state_dict(torch.load(\"net_before_pruning.pt\"))\n","\n","# Comment if you have loaded pretrained weights\n","# Tune the hyperparameters here.\n","#train(net, epochs=50, batch_size=128, lr=0.01, reg=0.005)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZsYrt_MCyGVE","executionInfo":{"status":"ok","timestamp":1698443183397,"user_tz":240,"elapsed":18366,"user":{"displayName":"Amarnath Shinde","userId":"15881715752542230515"}},"outputId":"9ddd5ebe-6fd4-46ed-e622-e1fa202afdde"},"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","Test Loss=0.2860, Test accuracy=0.9145\n"]},{"output_type":"execute_result","data":{"text/plain":["0.9145"]},"metadata":{},"execution_count":8}],"source":["# Load the best weight paramters\n","net.load_state_dict(torch.load(\"net_before_pruning.pt\"))\n","test(net)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q5D8C9h1yGVE","executionInfo":{"status":"ok","timestamp":1698207147450,"user_tz":240,"elapsed":129,"user":{"displayName":"Amarnath Shinde","userId":"00393772111366414345"}},"outputId":"7c1981bf-7769-432d-8978-bf3c3158e0ae"},"outputs":[{"output_type":"stream","name":"stdout","text":["-----Summary before pruning-----\n","Layer id\tType\t\tParameter\tNon-zero parameter\tSparsity(\\%)\n","1\t\tConvolutional\t864\t\t864\t\t\t0.000000\n","2\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","3\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","4\t\tConvolutional\t9216\t\t9216\t\t\t0.000000\n","5\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","6\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","7\t\tConvolutional\t18432\t\t18432\t\t\t0.000000\n","8\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","9\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","10\t\tConvolutional\t36864\t\t36864\t\t\t0.000000\n","11\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","12\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","13\t\tConvolutional\t73728\t\t73728\t\t\t0.000000\n","14\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","15\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","16\t\tConvolutional\t147456\t\t147456\t\t\t0.000000\n","17\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","18\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","19\t\tConvolutional\t147456\t\t147456\t\t\t0.000000\n","20\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","21\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","22\t\tConvolutional\t294912\t\t294912\t\t\t0.000000\n","23\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","24\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","25\t\tConvolutional\t589824\t\t589824\t\t\t0.000000\n","26\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","27\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","28\t\tConvolutional\t589824\t\t589824\t\t\t0.000000\n","29\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","30\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","31\t\tConvolutional\t589824\t\t589824\t\t\t0.000000\n","32\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","33\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","34\t\tConvolutional\t589824\t\t589824\t\t\t0.000000\n","35\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","36\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","37\t\tConvolutional\t589824\t\t589824\t\t\t0.000000\n","38\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","39\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","40\t\tLinear\t\t65536\t\t65536\t\t\t0.000000\n","41\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","42\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","43\t\tLinear\t\t65536\t\t65536\t\t\t0.000000\n","44\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","45\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","46\t\tLinear\t\t2560\t\t2560\t\t\t0.000000\n","Total nonzero parameters: 3811680\n","Total parameters: 3811680\n","Total sparsity: 0.000000\n","-------------------------------\n"]}],"source":["print(\"-----Summary before pruning-----\")\n","summary(net)\n","print(\"-------------------------------\")"]},{"cell_type":"markdown","metadata":{"id":"00k3oTlVyGVF"},"source":["### Pruning & Finetune with pruned connections"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4pdiSJHZyGVF","executionInfo":{"status":"ok","timestamp":1698443199199,"user_tz":240,"elapsed":5215,"user":{"displayName":"Amarnath Shinde","userId":"15881715752542230515"}},"outputId":"7ece561c-5eb2-452a-bcb5-e89a589ef625"},"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","Test Loss=0.3427, Test accuracy=0.8970\n"]},{"output_type":"execute_result","data":{"text/plain":["0.897"]},"metadata":{},"execution_count":9}],"source":["# Test accuracy before fine-tuning\n","prune(net, method='std', q=45.0, s=0.5)\n","test(net)"]},{"cell_type":"code","source":["# Question 2(D):\n","\n","import matplotlib.pyplot as plt\n","\n","sensitivity_values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n","\n","sparsity_values = []\n","accuracy_drops = []\n","\n","for s in sensitivity_values:\n","  prune(net, method='std', q=45.0, s=s)\n","  Test_accuracy = test(net)\n","  Sparsity = summary(net)\n","  accuracy_drop = 0.9145 - Test_accuracy\n","\n","\n","\n","  sparsity_values.append(Sparsity)  # Assuming you store sparsity in 'sparsity'\n","  accuracy_drops.append(accuracy_drop)\n","\n","\n","\n","# Plot the relationship between sparsity and accuracy drop\n","plt.plot(sparsity_values, accuracy_drops, marker='o', linestyle='-')\n","plt.xlabel('Sparsity')\n","plt.ylabel('Accuracy Drop')\n","plt.title('Sparsity vs. Accuracy Drop')\n","plt.grid(True)\n","\n","for i, s in enumerate(sensitivity_values):\n","    plt.annotate(f's={s}', (sparsity_values[i], accuracy_drops[i]), textcoords=\"offset points\", xytext=(0,10), ha='center')\n","\n","plt.show()\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"Z4ZJrBENLN9X","executionInfo":{"status":"ok","timestamp":1698263131906,"user_tz":240,"elapsed":40599,"user":{"displayName":"Amarnath Shinde","userId":"00393772111366414345"}},"outputId":"09c511af-1fc8-4634-dd2e-08fa8a34a623"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","Test Loss=0.2858, Test accuracy=0.9149\n","Layer id\tType\t\tParameter\tNon-zero parameter\tSparsity(\\%)\n","1\t\tConvolutional\t864\t\t707\t\t\t0.181713\n","2\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","3\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","4\t\tConvolutional\t9216\t\t7746\t\t\t0.159505\n","5\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","6\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","7\t\tConvolutional\t18432\t\t16830\t\t\t0.086914\n","8\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","9\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","10\t\tConvolutional\t36864\t\t33793\t\t\t0.083306\n","11\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","12\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","13\t\tConvolutional\t73728\t\t67687\t\t\t0.081936\n","14\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","15\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","16\t\tConvolutional\t147456\t\t135628\t\t\t0.080214\n","17\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","18\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","19\t\tConvolutional\t147456\t\t135345\t\t\t0.082133\n","20\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","21\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","22\t\tConvolutional\t294912\t\t269008\t\t\t0.087836\n","23\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","24\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","25\t\tConvolutional\t589824\t\t535343\t\t\t0.092368\n","26\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","27\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","28\t\tConvolutional\t589824\t\t530349\t\t\t0.100835\n","29\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","30\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","31\t\tConvolutional\t589824\t\t508025\t\t\t0.138684\n","32\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","33\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","34\t\tConvolutional\t589824\t\t480186\t\t\t0.185883\n","35\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","36\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","37\t\tConvolutional\t589824\t\t444310\t\t\t0.246707\n","38\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","39\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","40\t\tLinear\t\t65536\t\t59870\t\t\t0.086456\n","41\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","42\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","43\t\tLinear\t\t65536\t\t59559\t\t\t0.091202\n","44\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","45\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","46\t\tLinear\t\t2560\t\t2445\t\t\t0.044922\n","Total nonzero parameters: 3286831\n","Total parameters: 3811680\n","Total sparsity: 0.137695\n","Files already downloaded and verified\n","Test Loss=0.2878, Test accuracy=0.9136\n","Layer id\tType\t\tParameter\tNon-zero parameter\tSparsity(\\%)\n","1\t\tConvolutional\t864\t\t603\t\t\t0.302083\n","2\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","3\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","4\t\tConvolutional\t9216\t\t6514\t\t\t0.293186\n","5\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","6\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","7\t\tConvolutional\t18432\t\t15168\t\t\t0.177083\n","8\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","9\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","10\t\tConvolutional\t36864\t\t30826\t\t\t0.163791\n","11\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","12\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","13\t\tConvolutional\t73728\t\t61625\t\t\t0.164157\n","14\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","15\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","16\t\tConvolutional\t147456\t\t123733\t\t\t0.160882\n","17\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","18\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","19\t\tConvolutional\t147456\t\t123336\t\t\t0.163574\n","20\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","21\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","22\t\tConvolutional\t294912\t\t243670\t\t\t0.173754\n","23\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","24\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","25\t\tConvolutional\t589824\t\t481919\t\t\t0.182944\n","26\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","27\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","28\t\tConvolutional\t589824\t\t472186\t\t\t0.199446\n","29\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","30\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","31\t\tConvolutional\t589824\t\t435247\t\t\t0.262073\n","32\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","33\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","34\t\tConvolutional\t589824\t\t392471\t\t\t0.334596\n","35\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","36\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","37\t\tConvolutional\t589824\t\t339551\t\t\t0.424318\n","38\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","39\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","40\t\tLinear\t\t65536\t\t54292\t\t\t0.171570\n","41\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","42\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","43\t\tLinear\t\t65536\t\t53082\t\t\t0.190033\n","44\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","45\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","46\t\tLinear\t\t2560\t\t2298\t\t\t0.102344\n","Total nonzero parameters: 2836521\n","Total parameters: 3811680\n","Total sparsity: 0.255834\n","Files already downloaded and verified\n","Test Loss=0.2940, Test accuracy=0.9090\n","Layer id\tType\t\tParameter\tNon-zero parameter\tSparsity(\\%)\n","1\t\tConvolutional\t864\t\t514\t\t\t0.405093\n","2\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","3\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","4\t\tConvolutional\t9216\t\t5539\t\t\t0.398980\n","5\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","6\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","7\t\tConvolutional\t18432\t\t13612\t\t\t0.261502\n","8\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","9\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","10\t\tConvolutional\t36864\t\t27829\t\t\t0.245090\n","11\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","12\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","13\t\tConvolutional\t73728\t\t55713\t\t\t0.244344\n","14\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","15\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","16\t\tConvolutional\t147456\t\t112423\t\t\t0.237583\n","17\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","18\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","19\t\tConvolutional\t147456\t\t111474\t\t\t0.244019\n","20\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","21\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","22\t\tConvolutional\t294912\t\t219028\t\t\t0.257311\n","23\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","24\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","25\t\tConvolutional\t589824\t\t430423\t\t\t0.270252\n","26\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","27\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","28\t\tConvolutional\t589824\t\t417429\t\t\t0.292282\n","29\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","30\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","31\t\tConvolutional\t589824\t\t374229\t\t\t0.365524\n","32\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","33\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","34\t\tConvolutional\t589824\t\t328010\t\t\t0.443885\n","35\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","36\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","37\t\tConvolutional\t589824\t\t270550\t\t\t0.541304\n","38\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","39\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","40\t\tLinear\t\t65536\t\t48909\t\t\t0.253708\n","41\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","42\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","43\t\tLinear\t\t65536\t\t46470\t\t\t0.290924\n","44\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","45\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","46\t\tLinear\t\t2560\t\t2021\t\t\t0.210547\n","Total nonzero parameters: 2464173\n","Total parameters: 3811680\n","Total sparsity: 0.353520\n","Files already downloaded and verified\n","Test Loss=0.3083, Test accuracy=0.9061\n","Layer id\tType\t\tParameter\tNon-zero parameter\tSparsity(\\%)\n","1\t\tConvolutional\t864\t\t458\t\t\t0.469907\n","2\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","3\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","4\t\tConvolutional\t9216\t\t4741\t\t\t0.485569\n","5\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","6\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","7\t\tConvolutional\t18432\t\t12147\t\t\t0.340983\n","8\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","9\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","10\t\tConvolutional\t36864\t\t25054\t\t\t0.320367\n","11\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","12\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","13\t\tConvolutional\t73728\t\t50094\t\t\t0.320557\n","14\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","15\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","16\t\tConvolutional\t147456\t\t101174\t\t\t0.313870\n","17\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","18\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","19\t\tConvolutional\t147456\t\t100132\t\t\t0.320936\n","20\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","21\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","22\t\tConvolutional\t294912\t\t195651\t\t\t0.336578\n","23\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","24\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","25\t\tConvolutional\t589824\t\t381911\t\t\t0.352500\n","26\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","27\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","28\t\tConvolutional\t589824\t\t367117\t\t\t0.377582\n","29\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","30\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","31\t\tConvolutional\t589824\t\t323156\t\t\t0.452115\n","32\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","33\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","34\t\tConvolutional\t589824\t\t279454\t\t\t0.526208\n","35\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","36\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","37\t\tConvolutional\t589824\t\t221329\t\t\t0.624754\n","38\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","39\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","40\t\tLinear\t\t65536\t\t43590\t\t\t0.334869\n","41\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","42\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","43\t\tLinear\t\t65536\t\t39946\t\t\t0.390472\n","44\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","45\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","46\t\tLinear\t\t2560\t\t1572\t\t\t0.385938\n","Total nonzero parameters: 2147526\n","Total parameters: 3811680\n","Total sparsity: 0.436593\n","Files already downloaded and verified\n","Test Loss=0.3392, Test accuracy=0.8958\n","Layer id\tType\t\tParameter\tNon-zero parameter\tSparsity(\\%)\n","1\t\tConvolutional\t864\t\t389\t\t\t0.549769\n","2\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","3\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","4\t\tConvolutional\t9216\t\t4107\t\t\t0.554362\n","5\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","6\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","7\t\tConvolutional\t18432\t\t10806\t\t\t0.413737\n","8\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","9\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","10\t\tConvolutional\t36864\t\t22448\t\t\t0.391059\n","11\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","12\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","13\t\tConvolutional\t73728\t\t44726\t\t\t0.393365\n","14\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","15\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","16\t\tConvolutional\t147456\t\t90836\t\t\t0.383979\n","17\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","18\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","19\t\tConvolutional\t147456\t\t89400\t\t\t0.393717\n","20\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","21\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","22\t\tConvolutional\t294912\t\t174008\t\t\t0.409966\n","23\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","24\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","25\t\tConvolutional\t589824\t\t336848\t\t\t0.428901\n","26\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","27\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","28\t\tConvolutional\t589824\t\t321766\t\t\t0.454471\n","29\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","30\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","31\t\tConvolutional\t589824\t\t280940\t\t\t0.523688\n","32\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","33\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","34\t\tConvolutional\t589824\t\t241321\t\t\t0.590859\n","35\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","36\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","37\t\tConvolutional\t589824\t\t184810\t\t\t0.686669\n","38\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","39\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","40\t\tLinear\t\t65536\t\t38710\t\t\t0.409332\n","41\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","42\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","43\t\tLinear\t\t65536\t\t33940\t\t\t0.482117\n","44\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","45\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","46\t\tLinear\t\t2560\t\t1019\t\t\t0.601953\n","Total nonzero parameters: 1876074\n","Total parameters: 3811680\n","Total sparsity: 0.507809\n","Files already downloaded and verified\n","Test Loss=0.3968, Test accuracy=0.8839\n","Layer id\tType\t\tParameter\tNon-zero parameter\tSparsity(\\%)\n","1\t\tConvolutional\t864\t\t341\t\t\t0.605324\n","2\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","3\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","4\t\tConvolutional\t9216\t\t3517\t\t\t0.618381\n","5\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","6\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","7\t\tConvolutional\t18432\t\t9494\t\t\t0.484918\n","8\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","9\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","10\t\tConvolutional\t36864\t\t20055\t\t\t0.455973\n","11\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","12\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","13\t\tConvolutional\t73728\t\t39840\t\t\t0.459635\n","14\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","15\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","16\t\tConvolutional\t147456\t\t81144\t\t\t0.449707\n","17\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","18\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","19\t\tConvolutional\t147456\t\t79287\t\t\t0.462301\n","20\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","21\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","22\t\tConvolutional\t294912\t\t154058\t\t\t0.477614\n","23\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","24\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","25\t\tConvolutional\t589824\t\t296875\t\t\t0.496672\n","26\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","27\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","28\t\tConvolutional\t589824\t\t281501\t\t\t0.522737\n","29\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","30\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","31\t\tConvolutional\t589824\t\t245498\t\t\t0.583778\n","32\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","33\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","34\t\tConvolutional\t589824\t\t211148\t\t\t0.642015\n","35\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","36\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","37\t\tConvolutional\t589824\t\t157680\t\t\t0.732666\n","38\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","39\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","40\t\tLinear\t\t65536\t\t34273\t\t\t0.477036\n","41\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","42\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","43\t\tLinear\t\t65536\t\t28985\t\t\t0.557724\n","44\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","45\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","46\t\tLinear\t\t2560\t\t676\t\t\t0.735938\n","Total nonzero parameters: 1644372\n","Total parameters: 3811680\n","Total sparsity: 0.568597\n","Files already downloaded and verified\n","Test Loss=0.5089, Test accuracy=0.8532\n","Layer id\tType\t\tParameter\tNon-zero parameter\tSparsity(\\%)\n","1\t\tConvolutional\t864\t\t300\t\t\t0.652778\n","2\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","3\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","4\t\tConvolutional\t9216\t\t3062\t\t\t0.667752\n","5\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","6\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","7\t\tConvolutional\t18432\t\t8437\t\t\t0.542263\n","8\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","9\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","10\t\tConvolutional\t36864\t\t17807\t\t\t0.516954\n","11\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","12\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","13\t\tConvolutional\t73728\t\t35340\t\t\t0.520671\n","14\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","15\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","16\t\tConvolutional\t147456\t\t72399\t\t\t0.509013\n","17\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","18\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","19\t\tConvolutional\t147456\t\t70366\t\t\t0.522800\n","20\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","21\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","22\t\tConvolutional\t294912\t\t136181\t\t\t0.538232\n","23\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","24\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","25\t\tConvolutional\t589824\t\t261023\t\t\t0.557456\n","26\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","27\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","28\t\tConvolutional\t589824\t\t246233\t\t\t0.582531\n","29\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","30\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","31\t\tConvolutional\t589824\t\t215537\t\t\t0.634574\n","32\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","33\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","34\t\tConvolutional\t589824\t\t186468\t\t\t0.683858\n","35\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","36\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","37\t\tConvolutional\t589824\t\t137048\t\t\t0.767646\n","38\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","39\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","40\t\tLinear\t\t65536\t\t30365\t\t\t0.536667\n","41\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","42\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","43\t\tLinear\t\t65536\t\t24900\t\t\t0.620056\n","44\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","45\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","46\t\tLinear\t\t2560\t\t473\t\t\t0.815234\n","Total nonzero parameters: 1445939\n","Total parameters: 3811680\n","Total sparsity: 0.620656\n","Files already downloaded and verified\n","Test Loss=0.5997, Test accuracy=0.8399\n","Layer id\tType\t\tParameter\tNon-zero parameter\tSparsity(\\%)\n","1\t\tConvolutional\t864\t\t270\t\t\t0.687500\n","2\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","3\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","4\t\tConvolutional\t9216\t\t2658\t\t\t0.711589\n","5\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","6\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","7\t\tConvolutional\t18432\t\t7440\t\t\t0.596354\n","8\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","9\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","10\t\tConvolutional\t36864\t\t15843\t\t\t0.570231\n","11\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","12\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","13\t\tConvolutional\t73728\t\t31344\t\t\t0.574870\n","14\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","15\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","16\t\tConvolutional\t147456\t\t64305\t\t\t0.563904\n","17\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","18\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","19\t\tConvolutional\t147456\t\t62375\t\t\t0.576992\n","20\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","21\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","22\t\tConvolutional\t294912\t\t120411\t\t\t0.591705\n","23\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","24\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","25\t\tConvolutional\t589824\t\t229986\t\t\t0.610077\n","26\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","27\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","28\t\tConvolutional\t589824\t\t216213\t\t\t0.633428\n","29\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","30\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","31\t\tConvolutional\t589824\t\t190383\t\t\t0.677221\n","32\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","33\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","34\t\tConvolutional\t589824\t\t166078\t\t\t0.718428\n","35\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","36\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","37\t\tConvolutional\t589824\t\t120953\t\t\t0.794934\n","38\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","39\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","40\t\tLinear\t\t65536\t\t26923\t\t\t0.589188\n","41\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","42\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","43\t\tLinear\t\t65536\t\t21502\t\t\t0.671906\n","44\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","45\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","46\t\tLinear\t\t2560\t\t389\t\t\t0.848047\n","Total nonzero parameters: 1277073\n","Total parameters: 3811680\n","Total sparsity: 0.664958\n","Files already downloaded and verified\n","Test Loss=0.8224, Test accuracy=0.7730\n","Layer id\tType\t\tParameter\tNon-zero parameter\tSparsity(\\%)\n","1\t\tConvolutional\t864\t\t246\t\t\t0.715278\n","2\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","3\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","4\t\tConvolutional\t9216\t\t2350\t\t\t0.745009\n","5\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","6\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","7\t\tConvolutional\t18432\t\t6578\t\t\t0.643121\n","8\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","9\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","10\t\tConvolutional\t36864\t\t14096\t\t\t0.617622\n","11\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","12\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","13\t\tConvolutional\t73728\t\t27856\t\t\t0.622179\n","14\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","15\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","16\t\tConvolutional\t147456\t\t57180\t\t\t0.612223\n","17\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","18\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","19\t\tConvolutional\t147456\t\t55273\t\t\t0.625156\n","20\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","21\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","22\t\tConvolutional\t294912\t\t106627\t\t\t0.638445\n","23\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","24\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","25\t\tConvolutional\t589824\t\t203443\t\t\t0.655078\n","26\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","27\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","28\t\tConvolutional\t589824\t\t190506\t\t\t0.677012\n","29\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","30\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","31\t\tConvolutional\t589824\t\t169197\t\t\t0.713140\n","32\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","33\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","34\t\tConvolutional\t589824\t\t149039\t\t\t0.747316\n","35\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","36\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","37\t\tConvolutional\t589824\t\t107886\t\t\t0.817088\n","38\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","39\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","40\t\tLinear\t\t65536\t\t23794\t\t\t0.636932\n","41\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","42\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","43\t\tLinear\t\t65536\t\t18686\t\t\t0.714874\n","44\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","45\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","46\t\tLinear\t\t2560\t\t364\t\t\t0.857812\n","Total nonzero parameters: 1133121\n","Total parameters: 3811680\n","Total sparsity: 0.702724\n","Files already downloaded and verified\n","Test Loss=1.1617, Test accuracy=0.6501\n","Layer id\tType\t\tParameter\tNon-zero parameter\tSparsity(\\%)\n","1\t\tConvolutional\t864\t\t217\t\t\t0.748843\n","2\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","3\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","4\t\tConvolutional\t9216\t\t2089\t\t\t0.773329\n","5\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","6\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","7\t\tConvolutional\t18432\t\t5811\t\t\t0.684733\n","8\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","9\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","10\t\tConvolutional\t36864\t\t12541\t\t\t0.659804\n","11\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","12\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","13\t\tConvolutional\t73728\t\t24811\t\t\t0.663479\n","14\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","15\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","16\t\tConvolutional\t147456\t\t50890\t\t\t0.654880\n","17\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","18\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","19\t\tConvolutional\t147456\t\t48974\t\t\t0.667874\n","20\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","21\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","22\t\tConvolutional\t294912\t\t94677\t\t\t0.678965\n","23\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","24\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","25\t\tConvolutional\t589824\t\t180254\t\t\t0.694394\n","26\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","27\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","28\t\tConvolutional\t589824\t\t168825\t\t\t0.713771\n","29\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","30\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","31\t\tConvolutional\t589824\t\t151099\t\t\t0.743824\n","32\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","33\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","34\t\tConvolutional\t589824\t\t134172\t\t\t0.772522\n","35\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","36\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","37\t\tConvolutional\t589824\t\t97081\t\t\t0.835407\n","38\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","39\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","40\t\tLinear\t\t65536\t\t21131\t\t\t0.677567\n","41\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","42\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","43\t\tLinear\t\t65536\t\t16412\t\t\t0.749573\n","44\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","45\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","46\t\tLinear\t\t2560\t\t353\t\t\t0.862109\n","Total nonzero parameters: 1009337\n","Total parameters: 3811680\n","Total sparsity: 0.735199\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b4w5WWsyyGVF","executionInfo":{"status":"ok","timestamp":1698444106290,"user_tz":240,"elapsed":893015,"user":{"displayName":"Amarnath Shinde","userId":"15881715752542230515"}},"outputId":"3523a45f-5bf4-4f85-ce13-6fe74d486536"},"outputs":[{"output_type":"stream","name":"stdout","text":["==> Preparing data..\n","Files already downloaded and verified\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","\n","Epoch: 0\n","[Step=16]\tLoss=0.1634\tacc=0.9585\t570.9 examples/second\n","[Step=32]\tLoss=0.1555\tacc=0.9614\t2443.7 examples/second\n","[Step=48]\tLoss=0.1550\tacc=0.9629\t2534.6 examples/second\n","[Step=64]\tLoss=0.1559\tacc=0.9617\t2517.8 examples/second\n","[Step=80]\tLoss=0.1548\tacc=0.9618\t2498.4 examples/second\n","[Step=96]\tLoss=0.1556\tacc=0.9613\t2510.3 examples/second\n","[Step=112]\tLoss=0.1563\tacc=0.9614\t2569.9 examples/second\n","[Step=128]\tLoss=0.1595\tacc=0.9599\t2051.9 examples/second\n","[Step=144]\tLoss=0.1580\tacc=0.9604\t1510.2 examples/second\n","[Step=160]\tLoss=0.1572\tacc=0.9606\t1704.0 examples/second\n","[Step=176]\tLoss=0.1576\tacc=0.9602\t1821.9 examples/second\n","[Step=192]\tLoss=0.1564\tacc=0.9605\t1492.9 examples/second\n","[Step=208]\tLoss=0.1573\tacc=0.9602\t2159.8 examples/second\n","[Step=224]\tLoss=0.1589\tacc=0.9595\t2596.0 examples/second\n","[Step=240]\tLoss=0.1600\tacc=0.9590\t2440.8 examples/second\n","[Step=256]\tLoss=0.1600\tacc=0.9590\t2530.6 examples/second\n","[Step=272]\tLoss=0.1594\tacc=0.9592\t2596.8 examples/second\n","[Step=288]\tLoss=0.1589\tacc=0.9594\t2604.3 examples/second\n","[Step=304]\tLoss=0.1575\tacc=0.9599\t2454.6 examples/second\n","[Step=320]\tLoss=0.1581\tacc=0.9596\t2557.2 examples/second\n","[Step=336]\tLoss=0.1580\tacc=0.9596\t2833.0 examples/second\n","[Step=352]\tLoss=0.1586\tacc=0.9593\t2421.7 examples/second\n","[Step=368]\tLoss=0.1588\tacc=0.9592\t3437.0 examples/second\n","[Step=384]\tLoss=0.1584\tacc=0.9593\t6142.5 examples/second\n","Test Loss=0.3018, Test acc=0.9100\n","Saving...\n","\n","Epoch: 1\n","[Step=400]\tLoss=0.1496\tacc=0.9618\t245.3 examples/second\n","[Step=416]\tLoss=0.1526\tacc=0.9628\t2139.3 examples/second\n","[Step=432]\tLoss=0.1503\tacc=0.9634\t2403.1 examples/second\n","[Step=448]\tLoss=0.1529\tacc=0.9626\t2434.1 examples/second\n","[Step=464]\tLoss=0.1578\tacc=0.9613\t2337.8 examples/second\n","[Step=480]\tLoss=0.1581\tacc=0.9599\t2407.1 examples/second\n","[Step=496]\tLoss=0.1585\tacc=0.9594\t2555.7 examples/second\n","[Step=512]\tLoss=0.1594\tacc=0.9589\t2539.2 examples/second\n","[Step=528]\tLoss=0.1598\tacc=0.9591\t2263.2 examples/second\n","[Step=544]\tLoss=0.1613\tacc=0.9587\t1550.1 examples/second\n","[Step=560]\tLoss=0.1605\tacc=0.9591\t1492.8 examples/second\n","[Step=576]\tLoss=0.1603\tacc=0.9590\t1597.6 examples/second\n","[Step=592]\tLoss=0.1623\tacc=0.9583\t1478.9 examples/second\n","[Step=608]\tLoss=0.1617\tacc=0.9583\t1925.4 examples/second\n","[Step=624]\tLoss=0.1618\tacc=0.9581\t2528.2 examples/second\n","[Step=640]\tLoss=0.1613\tacc=0.9582\t2447.6 examples/second\n","[Step=656]\tLoss=0.1610\tacc=0.9583\t2596.3 examples/second\n","[Step=672]\tLoss=0.1604\tacc=0.9585\t2387.2 examples/second\n","[Step=688]\tLoss=0.1610\tacc=0.9581\t2498.1 examples/second\n","[Step=704]\tLoss=0.1610\tacc=0.9580\t2492.6 examples/second\n","[Step=720]\tLoss=0.1602\tacc=0.9583\t2399.0 examples/second\n","[Step=736]\tLoss=0.1602\tacc=0.9584\t2575.5 examples/second\n","[Step=752]\tLoss=0.1612\tacc=0.9580\t2459.8 examples/second\n","[Step=768]\tLoss=0.1617\tacc=0.9580\t5802.5 examples/second\n","Test Loss=0.3023, Test acc=0.9096\n","\n","Epoch: 2\n","[Step=784]\tLoss=0.1530\tacc=0.9492\t315.6 examples/second\n","[Step=800]\tLoss=0.1496\tacc=0.9583\t1348.1 examples/second\n","[Step=816]\tLoss=0.1531\tacc=0.9598\t2582.4 examples/second\n","[Step=832]\tLoss=0.1572\tacc=0.9578\t2538.1 examples/second\n","[Step=848]\tLoss=0.1620\tacc=0.9557\t2308.7 examples/second\n","[Step=864]\tLoss=0.1616\tacc=0.9566\t2550.6 examples/second\n","[Step=880]\tLoss=0.1624\tacc=0.9561\t2381.7 examples/second\n","[Step=896]\tLoss=0.1635\tacc=0.9568\t2451.6 examples/second\n","[Step=912]\tLoss=0.1652\tacc=0.9563\t2515.9 examples/second\n","[Step=928]\tLoss=0.1634\tacc=0.9568\t2379.5 examples/second\n","[Step=944]\tLoss=0.1627\tacc=0.9573\t2455.8 examples/second\n","[Step=960]\tLoss=0.1629\tacc=0.9569\t1981.0 examples/second\n","[Step=976]\tLoss=0.1610\tacc=0.9574\t1535.9 examples/second\n","[Step=992]\tLoss=0.1618\tacc=0.9573\t1647.1 examples/second\n","[Step=1008]\tLoss=0.1609\tacc=0.9578\t1525.4 examples/second\n","[Step=1024]\tLoss=0.1602\tacc=0.9578\t1495.2 examples/second\n","[Step=1040]\tLoss=0.1612\tacc=0.9575\t2546.5 examples/second\n","[Step=1056]\tLoss=0.1607\tacc=0.9577\t2457.5 examples/second\n","[Step=1072]\tLoss=0.1605\tacc=0.9581\t2535.1 examples/second\n","[Step=1088]\tLoss=0.1615\tacc=0.9577\t2513.8 examples/second\n","[Step=1104]\tLoss=0.1613\tacc=0.9579\t2303.9 examples/second\n","[Step=1120]\tLoss=0.1609\tacc=0.9580\t2553.4 examples/second\n","[Step=1136]\tLoss=0.1609\tacc=0.9582\t2356.2 examples/second\n","[Step=1152]\tLoss=0.1607\tacc=0.9583\t3749.9 examples/second\n","[Step=1168]\tLoss=0.1604\tacc=0.9582\t5832.4 examples/second\n","Test Loss=0.3026, Test acc=0.9081\n","\n","Epoch: 3\n","[Step=1184]\tLoss=0.1475\tacc=0.9645\t281.4 examples/second\n","[Step=1200]\tLoss=0.1605\tacc=0.9592\t1664.4 examples/second\n","[Step=1216]\tLoss=0.1644\tacc=0.9580\t2593.0 examples/second\n","[Step=1232]\tLoss=0.1627\tacc=0.9580\t2488.1 examples/second\n","[Step=1248]\tLoss=0.1597\tacc=0.9592\t2186.2 examples/second\n","[Step=1264]\tLoss=0.1594\tacc=0.9596\t2437.7 examples/second\n","[Step=1280]\tLoss=0.1582\tacc=0.9601\t2325.6 examples/second\n","[Step=1296]\tLoss=0.1578\tacc=0.9602\t2703.4 examples/second\n","[Step=1312]\tLoss=0.1580\tacc=0.9604\t2260.5 examples/second\n","[Step=1328]\tLoss=0.1583\tacc=0.9603\t2522.9 examples/second\n","[Step=1344]\tLoss=0.1578\tacc=0.9608\t1795.3 examples/second\n","[Step=1360]\tLoss=0.1587\tacc=0.9604\t1843.6 examples/second\n","[Step=1376]\tLoss=0.1598\tacc=0.9596\t1570.0 examples/second\n","[Step=1392]\tLoss=0.1597\tacc=0.9595\t1185.0 examples/second\n","[Step=1408]\tLoss=0.1605\tacc=0.9590\t1523.8 examples/second\n","[Step=1424]\tLoss=0.1604\tacc=0.9593\t1453.0 examples/second\n","[Step=1440]\tLoss=0.1610\tacc=0.9589\t1912.0 examples/second\n","[Step=1456]\tLoss=0.1617\tacc=0.9588\t2544.8 examples/second\n","[Step=1472]\tLoss=0.1608\tacc=0.9591\t2473.8 examples/second\n","[Step=1488]\tLoss=0.1617\tacc=0.9588\t2409.2 examples/second\n","[Step=1504]\tLoss=0.1618\tacc=0.9589\t2569.9 examples/second\n","[Step=1520]\tLoss=0.1626\tacc=0.9586\t2476.9 examples/second\n","[Step=1536]\tLoss=0.1622\tacc=0.9586\t2708.3 examples/second\n","[Step=1552]\tLoss=0.1616\tacc=0.9588\t5853.1 examples/second\n","Test Loss=0.3004, Test acc=0.9100\n","\n","Epoch: 4\n","[Step=1568]\tLoss=0.1519\tacc=0.9570\t335.8 examples/second\n","[Step=1584]\tLoss=0.1429\tacc=0.9641\t942.8 examples/second\n","[Step=1600]\tLoss=0.1519\tacc=0.9592\t1476.4 examples/second\n","[Step=1616]\tLoss=0.1535\tacc=0.9591\t1628.1 examples/second\n","[Step=1632]\tLoss=0.1539\tacc=0.9592\t2485.1 examples/second\n","[Step=1648]\tLoss=0.1578\tacc=0.9573\t2548.3 examples/second\n","[Step=1664]\tLoss=0.1588\tacc=0.9574\t2347.3 examples/second\n","[Step=1680]\tLoss=0.1580\tacc=0.9583\t2497.6 examples/second\n","[Step=1696]\tLoss=0.1567\tacc=0.9588\t2441.8 examples/second\n","[Step=1712]\tLoss=0.1589\tacc=0.9571\t2446.9 examples/second\n","[Step=1728]\tLoss=0.1590\tacc=0.9567\t2616.0 examples/second\n","[Step=1744]\tLoss=0.1583\tacc=0.9574\t2159.4 examples/second\n","[Step=1760]\tLoss=0.1589\tacc=0.9571\t2584.2 examples/second\n","[Step=1776]\tLoss=0.1596\tacc=0.9567\t2354.5 examples/second\n","[Step=1792]\tLoss=0.1604\tacc=0.9567\t2514.8 examples/second\n","[Step=1808]\tLoss=0.1603\tacc=0.9567\t2283.7 examples/second\n","[Step=1824]\tLoss=0.1604\tacc=0.9569\t1498.7 examples/second\n","[Step=1840]\tLoss=0.1597\tacc=0.9575\t1503.9 examples/second\n","[Step=1856]\tLoss=0.1608\tacc=0.9568\t1609.7 examples/second\n","[Step=1872]\tLoss=0.1610\tacc=0.9569\t1613.1 examples/second\n","[Step=1888]\tLoss=0.1617\tacc=0.9565\t1947.3 examples/second\n","[Step=1904]\tLoss=0.1613\tacc=0.9567\t2305.4 examples/second\n","[Step=1920]\tLoss=0.1615\tacc=0.9567\t2578.0 examples/second\n","[Step=1936]\tLoss=0.1605\tacc=0.9571\t4014.7 examples/second\n","[Step=1952]\tLoss=0.1608\tacc=0.9570\t5718.6 examples/second\n","Test Loss=0.3026, Test acc=0.9087\n","\n","Epoch: 5\n","[Step=1968]\tLoss=0.1712\tacc=0.9507\t354.9 examples/second\n","[Step=1984]\tLoss=0.1636\tacc=0.9545\t2368.0 examples/second\n","[Step=2000]\tLoss=0.1651\tacc=0.9540\t1919.6 examples/second\n","[Step=2016]\tLoss=0.1645\tacc=0.9547\t1435.5 examples/second\n","[Step=2032]\tLoss=0.1635\tacc=0.9552\t1315.0 examples/second\n","[Step=2048]\tLoss=0.1632\tacc=0.9557\t1435.3 examples/second\n","[Step=2064]\tLoss=0.1616\tacc=0.9564\t1283.6 examples/second\n","[Step=2080]\tLoss=0.1609\tacc=0.9571\t1393.7 examples/second\n","[Step=2096]\tLoss=0.1615\tacc=0.9567\t1354.2 examples/second\n","[Step=2112]\tLoss=0.1627\tacc=0.9564\t1343.9 examples/second\n","[Step=2128]\tLoss=0.1629\tacc=0.9565\t1853.0 examples/second\n","[Step=2144]\tLoss=0.1626\tacc=0.9567\t2539.4 examples/second\n","[Step=2160]\tLoss=0.1629\tacc=0.9565\t1805.5 examples/second\n","[Step=2176]\tLoss=0.1638\tacc=0.9561\t1447.2 examples/second\n","[Step=2192]\tLoss=0.1630\tacc=0.9562\t1584.2 examples/second\n","[Step=2208]\tLoss=0.1632\tacc=0.9563\t1571.2 examples/second\n","[Step=2224]\tLoss=0.1628\tacc=0.9564\t1648.9 examples/second\n","[Step=2240]\tLoss=0.1628\tacc=0.9566\t2351.8 examples/second\n","[Step=2256]\tLoss=0.1624\tacc=0.9568\t2523.1 examples/second\n","[Step=2272]\tLoss=0.1618\tacc=0.9569\t1952.6 examples/second\n","[Step=2288]\tLoss=0.1614\tacc=0.9573\t1533.8 examples/second\n","[Step=2304]\tLoss=0.1611\tacc=0.9573\t1478.9 examples/second\n","[Step=2320]\tLoss=0.1625\tacc=0.9569\t1926.0 examples/second\n","[Step=2336]\tLoss=0.1626\tacc=0.9566\t5489.1 examples/second\n","Test Loss=0.3011, Test acc=0.9095\n","\n","Epoch: 6\n","[Step=2352]\tLoss=0.1386\tacc=0.9661\t369.0 examples/second\n","[Step=2368]\tLoss=0.1636\tacc=0.9585\t1639.6 examples/second\n","[Step=2384]\tLoss=0.1608\tacc=0.9593\t2523.3 examples/second\n","[Step=2400]\tLoss=0.1636\tacc=0.9582\t2388.9 examples/second\n","[Step=2416]\tLoss=0.1616\tacc=0.9588\t2050.1 examples/second\n","[Step=2432]\tLoss=0.1616\tacc=0.9580\t2202.7 examples/second\n","[Step=2448]\tLoss=0.1612\tacc=0.9585\t2177.3 examples/second\n","[Step=2464]\tLoss=0.1611\tacc=0.9586\t1490.5 examples/second\n","[Step=2480]\tLoss=0.1591\tacc=0.9597\t1440.8 examples/second\n","[Step=2496]\tLoss=0.1573\tacc=0.9603\t1647.9 examples/second\n","[Step=2512]\tLoss=0.1586\tacc=0.9594\t1499.0 examples/second\n","[Step=2528]\tLoss=0.1567\tacc=0.9599\t2109.7 examples/second\n","[Step=2544]\tLoss=0.1576\tacc=0.9594\t2482.4 examples/second\n","[Step=2560]\tLoss=0.1587\tacc=0.9588\t2468.8 examples/second\n","[Step=2576]\tLoss=0.1598\tacc=0.9583\t2544.3 examples/second\n","[Step=2592]\tLoss=0.1614\tacc=0.9578\t2631.7 examples/second\n","[Step=2608]\tLoss=0.1616\tacc=0.9578\t2343.0 examples/second\n","[Step=2624]\tLoss=0.1623\tacc=0.9575\t2369.7 examples/second\n","[Step=2640]\tLoss=0.1608\tacc=0.9582\t2479.9 examples/second\n","[Step=2656]\tLoss=0.1609\tacc=0.9582\t2373.5 examples/second\n","[Step=2672]\tLoss=0.1613\tacc=0.9581\t2535.1 examples/second\n","[Step=2688]\tLoss=0.1609\tacc=0.9582\t2307.1 examples/second\n","[Step=2704]\tLoss=0.1611\tacc=0.9578\t2569.1 examples/second\n","[Step=2720]\tLoss=0.1608\tacc=0.9579\t4119.9 examples/second\n","[Step=2736]\tLoss=0.1606\tacc=0.9579\t5520.7 examples/second\n","Test Loss=0.3044, Test acc=0.9099\n","\n","Epoch: 7\n","[Step=2752]\tLoss=0.1777\tacc=0.9531\t282.2 examples/second\n","[Step=2768]\tLoss=0.1692\tacc=0.9561\t2355.4 examples/second\n","[Step=2784]\tLoss=0.1669\tacc=0.9561\t2614.4 examples/second\n","[Step=2800]\tLoss=0.1635\tacc=0.9572\t2230.6 examples/second\n","[Step=2816]\tLoss=0.1632\tacc=0.9582\t2479.0 examples/second\n","[Step=2832]\tLoss=0.1638\tacc=0.9576\t2440.7 examples/second\n","[Step=2848]\tLoss=0.1626\tacc=0.9589\t2147.1 examples/second\n","[Step=2864]\tLoss=0.1616\tacc=0.9593\t1950.9 examples/second\n","[Step=2880]\tLoss=0.1608\tacc=0.9595\t1406.5 examples/second\n","[Step=2896]\tLoss=0.1608\tacc=0.9591\t1474.5 examples/second\n","[Step=2912]\tLoss=0.1629\tacc=0.9585\t1565.7 examples/second\n","[Step=2928]\tLoss=0.1616\tacc=0.9589\t1507.5 examples/second\n","[Step=2944]\tLoss=0.1625\tacc=0.9585\t2338.0 examples/second\n","[Step=2960]\tLoss=0.1620\tacc=0.9584\t2567.0 examples/second\n","[Step=2976]\tLoss=0.1617\tacc=0.9583\t2439.4 examples/second\n","[Step=2992]\tLoss=0.1603\tacc=0.9587\t2407.2 examples/second\n","[Step=3008]\tLoss=0.1596\tacc=0.9588\t2494.3 examples/second\n","[Step=3024]\tLoss=0.1604\tacc=0.9584\t2317.9 examples/second\n","[Step=3040]\tLoss=0.1602\tacc=0.9585\t2528.1 examples/second\n","[Step=3056]\tLoss=0.1609\tacc=0.9581\t2410.3 examples/second\n","[Step=3072]\tLoss=0.1619\tacc=0.9577\t2435.5 examples/second\n","[Step=3088]\tLoss=0.1619\tacc=0.9577\t2423.6 examples/second\n","[Step=3104]\tLoss=0.1614\tacc=0.9578\t3165.7 examples/second\n","[Step=3120]\tLoss=0.1612\tacc=0.9579\t5611.7 examples/second\n","Test Loss=0.3009, Test acc=0.9102\n","Saving...\n","\n","Epoch: 8\n","[Step=3136]\tLoss=0.1641\tacc=0.9580\t281.9 examples/second\n","[Step=3152]\tLoss=0.1558\tacc=0.9629\t2125.6 examples/second\n","[Step=3168]\tLoss=0.1624\tacc=0.9572\t2356.4 examples/second\n","[Step=3184]\tLoss=0.1652\tacc=0.9563\t2471.5 examples/second\n","[Step=3200]\tLoss=0.1634\tacc=0.9563\t2321.8 examples/second\n","[Step=3216]\tLoss=0.1608\tacc=0.9574\t2406.5 examples/second\n","[Step=3232]\tLoss=0.1595\tacc=0.9583\t2347.9 examples/second\n","[Step=3248]\tLoss=0.1607\tacc=0.9577\t2473.9 examples/second\n","[Step=3264]\tLoss=0.1621\tacc=0.9572\t2397.6 examples/second\n","[Step=3280]\tLoss=0.1620\tacc=0.9571\t2050.8 examples/second\n","[Step=3296]\tLoss=0.1631\tacc=0.9568\t1554.9 examples/second\n","[Step=3312]\tLoss=0.1624\tacc=0.9573\t1504.4 examples/second\n","[Step=3328]\tLoss=0.1616\tacc=0.9577\t1538.5 examples/second\n","[Step=3344]\tLoss=0.1607\tacc=0.9581\t1607.7 examples/second\n","[Step=3360]\tLoss=0.1621\tacc=0.9573\t1684.5 examples/second\n","[Step=3376]\tLoss=0.1619\tacc=0.9571\t2441.4 examples/second\n","[Step=3392]\tLoss=0.1625\tacc=0.9570\t2460.3 examples/second\n","[Step=3408]\tLoss=0.1618\tacc=0.9573\t2592.6 examples/second\n","[Step=3424]\tLoss=0.1608\tacc=0.9576\t2341.9 examples/second\n","[Step=3440]\tLoss=0.1614\tacc=0.9576\t2477.3 examples/second\n","[Step=3456]\tLoss=0.1616\tacc=0.9575\t1898.7 examples/second\n","[Step=3472]\tLoss=0.1613\tacc=0.9574\t2635.8 examples/second\n","[Step=3488]\tLoss=0.1612\tacc=0.9573\t2380.2 examples/second\n","[Step=3504]\tLoss=0.1615\tacc=0.9571\t5533.2 examples/second\n","Test Loss=0.3021, Test acc=0.9102\n","\n","Epoch: 9\n","[Step=3520]\tLoss=0.1049\tacc=0.9922\t320.2 examples/second\n","[Step=3536]\tLoss=0.1477\tacc=0.9623\t1309.5 examples/second\n","[Step=3552]\tLoss=0.1441\tacc=0.9643\t2456.0 examples/second\n","[Step=3568]\tLoss=0.1453\tacc=0.9644\t2374.1 examples/second\n","[Step=3584]\tLoss=0.1522\tacc=0.9612\t2432.7 examples/second\n","[Step=3600]\tLoss=0.1546\tacc=0.9606\t2428.5 examples/second\n","[Step=3616]\tLoss=0.1534\tacc=0.9616\t2544.4 examples/second\n","[Step=3632]\tLoss=0.1539\tacc=0.9611\t2343.0 examples/second\n","[Step=3648]\tLoss=0.1552\tacc=0.9606\t2557.1 examples/second\n","[Step=3664]\tLoss=0.1556\tacc=0.9607\t2420.8 examples/second\n","[Step=3680]\tLoss=0.1559\tacc=0.9601\t2500.9 examples/second\n","[Step=3696]\tLoss=0.1563\tacc=0.9598\t2310.8 examples/second\n","[Step=3712]\tLoss=0.1572\tacc=0.9594\t1753.7 examples/second\n","[Step=3728]\tLoss=0.1571\tacc=0.9593\t1404.7 examples/second\n","[Step=3744]\tLoss=0.1573\tacc=0.9592\t1445.1 examples/second\n","[Step=3760]\tLoss=0.1568\tacc=0.9595\t1522.8 examples/second\n","[Step=3776]\tLoss=0.1567\tacc=0.9595\t1570.9 examples/second\n","[Step=3792]\tLoss=0.1575\tacc=0.9592\t2095.1 examples/second\n","[Step=3808]\tLoss=0.1575\tacc=0.9593\t2562.7 examples/second\n","[Step=3824]\tLoss=0.1574\tacc=0.9594\t2369.5 examples/second\n","[Step=3840]\tLoss=0.1575\tacc=0.9595\t2406.8 examples/second\n","[Step=3856]\tLoss=0.1582\tacc=0.9593\t2576.6 examples/second\n","[Step=3872]\tLoss=0.1591\tacc=0.9587\t2443.5 examples/second\n","[Step=3888]\tLoss=0.1594\tacc=0.9588\t3384.0 examples/second\n","[Step=3904]\tLoss=0.1595\tacc=0.9587\t5615.7 examples/second\n","Test Loss=0.3020, Test acc=0.9099\n","\n","Epoch: 10\n","[Step=3920]\tLoss=0.1420\tacc=0.9602\t319.0 examples/second\n","[Step=3936]\tLoss=0.1352\tacc=0.9645\t1199.2 examples/second\n","[Step=3952]\tLoss=0.1504\tacc=0.9596\t1580.6 examples/second\n","[Step=3968]\tLoss=0.1536\tacc=0.9612\t1780.9 examples/second\n","[Step=3984]\tLoss=0.1555\tacc=0.9600\t2406.9 examples/second\n","[Step=4000]\tLoss=0.1564\tacc=0.9589\t2585.9 examples/second\n","[Step=4016]\tLoss=0.1559\tacc=0.9590\t2373.5 examples/second\n","[Step=4032]\tLoss=0.1572\tacc=0.9587\t2674.8 examples/second\n","[Step=4048]\tLoss=0.1575\tacc=0.9583\t2335.5 examples/second\n","[Step=4064]\tLoss=0.1589\tacc=0.9577\t2404.3 examples/second\n","[Step=4080]\tLoss=0.1601\tacc=0.9572\t2621.8 examples/second\n","[Step=4096]\tLoss=0.1602\tacc=0.9572\t2385.4 examples/second\n","[Step=4112]\tLoss=0.1604\tacc=0.9575\t2558.0 examples/second\n","[Step=4128]\tLoss=0.1590\tacc=0.9581\t2569.6 examples/second\n","[Step=4144]\tLoss=0.1596\tacc=0.9579\t2376.3 examples/second\n","[Step=4160]\tLoss=0.1597\tacc=0.9578\t1533.2 examples/second\n","[Step=4176]\tLoss=0.1601\tacc=0.9578\t1575.6 examples/second\n","[Step=4192]\tLoss=0.1608\tacc=0.9578\t1497.5 examples/second\n","[Step=4208]\tLoss=0.1610\tacc=0.9579\t1551.9 examples/second\n","[Step=4224]\tLoss=0.1603\tacc=0.9580\t1567.3 examples/second\n","[Step=4240]\tLoss=0.1600\tacc=0.9580\t2320.6 examples/second\n","[Step=4256]\tLoss=0.1593\tacc=0.9582\t2622.5 examples/second\n","[Step=4272]\tLoss=0.1595\tacc=0.9580\t2569.6 examples/second\n","[Step=4288]\tLoss=0.1598\tacc=0.9578\t5662.8 examples/second\n","Test Loss=0.3011, Test acc=0.9099\n","\n","Epoch: 11\n","[Step=4304]\tLoss=0.1659\tacc=0.9479\t419.3 examples/second\n","[Step=4320]\tLoss=0.1708\tacc=0.9527\t1420.6 examples/second\n","[Step=4336]\tLoss=0.1617\tacc=0.9560\t2472.0 examples/second\n","[Step=4352]\tLoss=0.1691\tacc=0.9544\t1571.5 examples/second\n","[Step=4368]\tLoss=0.1687\tacc=0.9545\t1557.5 examples/second\n","[Step=4384]\tLoss=0.1637\tacc=0.9563\t1579.8 examples/second\n","[Step=4400]\tLoss=0.1625\tacc=0.9571\t1494.8 examples/second\n","[Step=4416]\tLoss=0.1632\tacc=0.9572\t1670.6 examples/second\n","[Step=4432]\tLoss=0.1630\tacc=0.9574\t2546.6 examples/second\n","[Step=4448]\tLoss=0.1616\tacc=0.9575\t1690.4 examples/second\n","[Step=4464]\tLoss=0.1592\tacc=0.9585\t1411.1 examples/second\n","[Step=4480]\tLoss=0.1577\tacc=0.9591\t1610.8 examples/second\n","[Step=4496]\tLoss=0.1577\tacc=0.9591\t1475.2 examples/second\n","[Step=4512]\tLoss=0.1578\tacc=0.9586\t1836.3 examples/second\n","[Step=4528]\tLoss=0.1577\tacc=0.9587\t2307.5 examples/second\n","[Step=4544]\tLoss=0.1582\tacc=0.9583\t1883.5 examples/second\n","[Step=4560]\tLoss=0.1587\tacc=0.9580\t2177.7 examples/second\n","[Step=4576]\tLoss=0.1580\tacc=0.9582\t1510.7 examples/second\n","[Step=4592]\tLoss=0.1584\tacc=0.9581\t1423.9 examples/second\n","[Step=4608]\tLoss=0.1587\tacc=0.9580\t1374.4 examples/second\n","[Step=4624]\tLoss=0.1593\tacc=0.9578\t1295.9 examples/second\n","[Step=4640]\tLoss=0.1593\tacc=0.9578\t1370.7 examples/second\n","[Step=4656]\tLoss=0.1590\tacc=0.9578\t1431.5 examples/second\n","[Step=4672]\tLoss=0.1591\tacc=0.9579\t2715.2 examples/second\n","[Step=4688]\tLoss=0.1594\tacc=0.9579\t5446.8 examples/second\n","Test Loss=0.3025, Test acc=0.9095\n","\n","Epoch: 12\n","[Step=4704]\tLoss=0.1765\tacc=0.9544\t340.0 examples/second\n","[Step=4720]\tLoss=0.1783\tacc=0.9551\t2367.4 examples/second\n","[Step=4736]\tLoss=0.1697\tacc=0.9576\t2643.5 examples/second\n","[Step=4752]\tLoss=0.1680\tacc=0.9559\t1645.3 examples/second\n","[Step=4768]\tLoss=0.1676\tacc=0.9557\t1416.5 examples/second\n","[Step=4784]\tLoss=0.1666\tacc=0.9559\t1592.8 examples/second\n","[Step=4800]\tLoss=0.1678\tacc=0.9552\t1512.6 examples/second\n","[Step=4816]\tLoss=0.1676\tacc=0.9555\t1781.3 examples/second\n","[Step=4832]\tLoss=0.1658\tacc=0.9560\t2372.3 examples/second\n","[Step=4848]\tLoss=0.1637\tacc=0.9567\t2526.9 examples/second\n","[Step=4864]\tLoss=0.1644\tacc=0.9567\t2574.3 examples/second\n","[Step=4880]\tLoss=0.1631\tacc=0.9573\t2404.6 examples/second\n","[Step=4896]\tLoss=0.1651\tacc=0.9567\t2406.8 examples/second\n","[Step=4912]\tLoss=0.1648\tacc=0.9569\t2327.7 examples/second\n","[Step=4928]\tLoss=0.1635\tacc=0.9575\t2007.9 examples/second\n","[Step=4944]\tLoss=0.1636\tacc=0.9575\t2280.6 examples/second\n","[Step=4960]\tLoss=0.1633\tacc=0.9576\t2457.7 examples/second\n","[Step=4976]\tLoss=0.1629\tacc=0.9575\t2529.0 examples/second\n","[Step=4992]\tLoss=0.1628\tacc=0.9575\t2326.4 examples/second\n","[Step=5008]\tLoss=0.1627\tacc=0.9575\t1790.4 examples/second\n","[Step=5024]\tLoss=0.1633\tacc=0.9573\t1514.7 examples/second\n","[Step=5040]\tLoss=0.1632\tacc=0.9574\t1510.9 examples/second\n","[Step=5056]\tLoss=0.1625\tacc=0.9574\t1831.8 examples/second\n","[Step=5072]\tLoss=0.1618\tacc=0.9576\t5246.2 examples/second\n","Test Loss=0.3032, Test acc=0.9094\n","\n","Epoch: 13\n","[Step=5088]\tLoss=0.1663\tacc=0.9578\t396.1 examples/second\n","[Step=5104]\tLoss=0.1612\tacc=0.9602\t1550.2 examples/second\n","[Step=5120]\tLoss=0.1720\tacc=0.9557\t2535.4 examples/second\n","[Step=5136]\tLoss=0.1664\tacc=0.9568\t2307.3 examples/second\n","[Step=5152]\tLoss=0.1643\tacc=0.9572\t2483.9 examples/second\n","[Step=5168]\tLoss=0.1620\tacc=0.9580\t2280.3 examples/second\n","[Step=5184]\tLoss=0.1645\tacc=0.9571\t2349.6 examples/second\n","[Step=5200]\tLoss=0.1627\tacc=0.9578\t1492.9 examples/second\n","[Step=5216]\tLoss=0.1610\tacc=0.9584\t1448.7 examples/second\n","[Step=5232]\tLoss=0.1610\tacc=0.9581\t1676.5 examples/second\n","[Step=5248]\tLoss=0.1617\tacc=0.9578\t1495.7 examples/second\n","[Step=5264]\tLoss=0.1616\tacc=0.9577\t1535.5 examples/second\n","[Step=5280]\tLoss=0.1600\tacc=0.9584\t2523.0 examples/second\n","[Step=5296]\tLoss=0.1616\tacc=0.9580\t2492.4 examples/second\n","[Step=5312]\tLoss=0.1637\tacc=0.9572\t2576.2 examples/second\n","[Step=5328]\tLoss=0.1632\tacc=0.9574\t2367.8 examples/second\n","[Step=5344]\tLoss=0.1636\tacc=0.9573\t1810.9 examples/second\n","[Step=5360]\tLoss=0.1637\tacc=0.9572\t2374.1 examples/second\n","[Step=5376]\tLoss=0.1623\tacc=0.9578\t2480.1 examples/second\n","[Step=5392]\tLoss=0.1628\tacc=0.9575\t2473.2 examples/second\n","[Step=5408]\tLoss=0.1625\tacc=0.9576\t2338.0 examples/second\n","[Step=5424]\tLoss=0.1625\tacc=0.9575\t2482.4 examples/second\n","[Step=5440]\tLoss=0.1625\tacc=0.9574\t2403.8 examples/second\n","[Step=5456]\tLoss=0.1624\tacc=0.9575\t4461.7 examples/second\n","[Step=5472]\tLoss=0.1619\tacc=0.9579\t5533.4 examples/second\n","Test Loss=0.3020, Test acc=0.9094\n","\n","Epoch: 14\n","[Step=5488]\tLoss=0.1514\tacc=0.9570\t283.1 examples/second\n","[Step=5504]\tLoss=0.1491\tacc=0.9604\t2241.0 examples/second\n","[Step=5520]\tLoss=0.1518\tacc=0.9603\t2475.2 examples/second\n","[Step=5536]\tLoss=0.1578\tacc=0.9587\t2423.1 examples/second\n","[Step=5552]\tLoss=0.1553\tacc=0.9587\t2300.1 examples/second\n","[Step=5568]\tLoss=0.1589\tacc=0.9576\t2342.4 examples/second\n","[Step=5584]\tLoss=0.1584\tacc=0.9577\t2607.6 examples/second\n","[Step=5600]\tLoss=0.1559\tacc=0.9586\t2291.4 examples/second\n","[Step=5616]\tLoss=0.1558\tacc=0.9590\t1745.2 examples/second\n","[Step=5632]\tLoss=0.1578\tacc=0.9582\t1518.1 examples/second\n","[Step=5648]\tLoss=0.1594\tacc=0.9579\t1621.5 examples/second\n","[Step=5664]\tLoss=0.1587\tacc=0.9581\t1503.3 examples/second\n","[Step=5680]\tLoss=0.1581\tacc=0.9585\t1529.9 examples/second\n","[Step=5696]\tLoss=0.1596\tacc=0.9579\t2732.4 examples/second\n","[Step=5712]\tLoss=0.1596\tacc=0.9579\t2348.5 examples/second\n","[Step=5728]\tLoss=0.1599\tacc=0.9580\t2483.0 examples/second\n","[Step=5744]\tLoss=0.1599\tacc=0.9581\t2303.7 examples/second\n","[Step=5760]\tLoss=0.1601\tacc=0.9582\t2522.3 examples/second\n","[Step=5776]\tLoss=0.1606\tacc=0.9581\t1816.2 examples/second\n","[Step=5792]\tLoss=0.1607\tacc=0.9581\t2560.1 examples/second\n","[Step=5808]\tLoss=0.1608\tacc=0.9580\t2416.1 examples/second\n","[Step=5824]\tLoss=0.1605\tacc=0.9581\t2546.5 examples/second\n","[Step=5840]\tLoss=0.1612\tacc=0.9577\t3178.0 examples/second\n","[Step=5856]\tLoss=0.1610\tacc=0.9578\t5712.9 examples/second\n","Test Loss=0.3016, Test acc=0.9105\n","Saving...\n","\n","Epoch: 15\n","[Step=5872]\tLoss=0.1610\tacc=0.9554\t288.9 examples/second\n","[Step=5888]\tLoss=0.1685\tacc=0.9555\t1810.6 examples/second\n","[Step=5904]\tLoss=0.1626\tacc=0.9557\t2476.3 examples/second\n","[Step=5920]\tLoss=0.1604\tacc=0.9563\t2429.8 examples/second\n","[Step=5936]\tLoss=0.1631\tacc=0.9557\t2338.5 examples/second\n","[Step=5952]\tLoss=0.1649\tacc=0.9555\t2543.8 examples/second\n","[Step=5968]\tLoss=0.1630\tacc=0.9564\t2243.6 examples/second\n","[Step=5984]\tLoss=0.1607\tacc=0.9567\t2514.3 examples/second\n","[Step=6000]\tLoss=0.1604\tacc=0.9569\t2619.1 examples/second\n","[Step=6016]\tLoss=0.1598\tacc=0.9577\t2327.8 examples/second\n","[Step=6032]\tLoss=0.1600\tacc=0.9579\t1804.9 examples/second\n","[Step=6048]\tLoss=0.1606\tacc=0.9582\t1471.1 examples/second\n","[Step=6064]\tLoss=0.1610\tacc=0.9578\t1612.3 examples/second\n","[Step=6080]\tLoss=0.1602\tacc=0.9582\t1571.9 examples/second\n","[Step=6096]\tLoss=0.1618\tacc=0.9576\t1586.9 examples/second\n","[Step=6112]\tLoss=0.1633\tacc=0.9573\t2479.9 examples/second\n","[Step=6128]\tLoss=0.1627\tacc=0.9574\t2490.3 examples/second\n","[Step=6144]\tLoss=0.1618\tacc=0.9578\t2473.2 examples/second\n","[Step=6160]\tLoss=0.1615\tacc=0.9578\t2411.4 examples/second\n","[Step=6176]\tLoss=0.1611\tacc=0.9580\t2386.2 examples/second\n","[Step=6192]\tLoss=0.1611\tacc=0.9581\t2567.7 examples/second\n","[Step=6208]\tLoss=0.1620\tacc=0.9577\t1922.8 examples/second\n","[Step=6224]\tLoss=0.1620\tacc=0.9577\t2675.1 examples/second\n","[Step=6240]\tLoss=0.1630\tacc=0.9572\t4557.3 examples/second\n","[Step=6256]\tLoss=0.1627\tacc=0.9575\t5784.7 examples/second\n","Test Loss=0.3018, Test acc=0.9096\n","\n","Epoch: 16\n","[Step=6272]\tLoss=0.1562\tacc=0.9614\t277.5 examples/second\n","[Step=6288]\tLoss=0.1570\tacc=0.9602\t2437.0 examples/second\n","[Step=6304]\tLoss=0.1645\tacc=0.9575\t2409.4 examples/second\n","[Step=6320]\tLoss=0.1648\tacc=0.9570\t2348.9 examples/second\n","[Step=6336]\tLoss=0.1620\tacc=0.9578\t2606.0 examples/second\n","[Step=6352]\tLoss=0.1623\tacc=0.9571\t2570.0 examples/second\n","[Step=6368]\tLoss=0.1615\tacc=0.9573\t2441.1 examples/second\n","[Step=6384]\tLoss=0.1614\tacc=0.9567\t2427.2 examples/second\n","[Step=6400]\tLoss=0.1612\tacc=0.9572\t2533.1 examples/second\n","[Step=6416]\tLoss=0.1598\tacc=0.9578\t2345.3 examples/second\n","[Step=6432]\tLoss=0.1594\tacc=0.9579\t2362.1 examples/second\n","[Step=6448]\tLoss=0.1594\tacc=0.9582\t2060.7 examples/second\n","[Step=6464]\tLoss=0.1580\tacc=0.9585\t1403.2 examples/second\n","[Step=6480]\tLoss=0.1582\tacc=0.9586\t1499.6 examples/second\n","[Step=6496]\tLoss=0.1582\tacc=0.9587\t1521.9 examples/second\n","[Step=6512]\tLoss=0.1580\tacc=0.9586\t1632.5 examples/second\n","[Step=6528]\tLoss=0.1588\tacc=0.9584\t2649.2 examples/second\n","[Step=6544]\tLoss=0.1595\tacc=0.9580\t2349.8 examples/second\n","[Step=6560]\tLoss=0.1590\tacc=0.9582\t2503.1 examples/second\n","[Step=6576]\tLoss=0.1597\tacc=0.9582\t2570.7 examples/second\n","[Step=6592]\tLoss=0.1595\tacc=0.9582\t2490.4 examples/second\n","[Step=6608]\tLoss=0.1605\tacc=0.9576\t2490.4 examples/second\n","[Step=6624]\tLoss=0.1591\tacc=0.9582\t3189.5 examples/second\n","[Step=6640]\tLoss=0.1584\tacc=0.9584\t5636.0 examples/second\n","Test Loss=0.3022, Test acc=0.9107\n","Saving...\n","\n","Epoch: 17\n","[Step=6656]\tLoss=0.1401\tacc=0.9696\t292.2 examples/second\n","[Step=6672]\tLoss=0.1577\tacc=0.9581\t1154.2 examples/second\n","[Step=6688]\tLoss=0.1660\tacc=0.9577\t2050.0 examples/second\n","[Step=6704]\tLoss=0.1624\tacc=0.9582\t2544.4 examples/second\n","[Step=6720]\tLoss=0.1592\tacc=0.9587\t2588.3 examples/second\n","[Step=6736]\tLoss=0.1588\tacc=0.9586\t2164.0 examples/second\n","[Step=6752]\tLoss=0.1585\tacc=0.9588\t2571.3 examples/second\n","[Step=6768]\tLoss=0.1576\tacc=0.9586\t2380.2 examples/second\n","[Step=6784]\tLoss=0.1592\tacc=0.9582\t2395.3 examples/second\n","[Step=6800]\tLoss=0.1588\tacc=0.9586\t2495.3 examples/second\n","[Step=6816]\tLoss=0.1590\tacc=0.9587\t2322.2 examples/second\n","[Step=6832]\tLoss=0.1599\tacc=0.9584\t2493.3 examples/second\n","[Step=6848]\tLoss=0.1582\tacc=0.9591\t2399.8 examples/second\n","[Step=6864]\tLoss=0.1586\tacc=0.9588\t2441.0 examples/second\n","[Step=6880]\tLoss=0.1582\tacc=0.9588\t1515.0 examples/second\n","[Step=6896]\tLoss=0.1590\tacc=0.9584\t1300.8 examples/second\n","[Step=6912]\tLoss=0.1590\tacc=0.9583\t1439.9 examples/second\n","[Step=6928]\tLoss=0.1598\tacc=0.9580\t1316.3 examples/second\n","[Step=6944]\tLoss=0.1595\tacc=0.9580\t1323.2 examples/second\n","[Step=6960]\tLoss=0.1598\tacc=0.9580\t1396.9 examples/second\n","[Step=6976]\tLoss=0.1598\tacc=0.9580\t1352.7 examples/second\n","[Step=6992]\tLoss=0.1602\tacc=0.9580\t2054.1 examples/second\n","[Step=7008]\tLoss=0.1603\tacc=0.9581\t1792.5 examples/second\n","[Step=7024]\tLoss=0.1599\tacc=0.9583\t5613.0 examples/second\n","Test Loss=0.3033, Test acc=0.9095\n","\n","Epoch: 18\n","[Step=7040]\tLoss=0.1385\tacc=0.9609\t315.4 examples/second\n","[Step=7056]\tLoss=0.1775\tacc=0.9527\t1171.3 examples/second\n","[Step=7072]\tLoss=0.1661\tacc=0.9589\t1455.8 examples/second\n","[Step=7088]\tLoss=0.1625\tacc=0.9598\t1485.4 examples/second\n","[Step=7104]\tLoss=0.1641\tacc=0.9585\t1527.6 examples/second\n","[Step=7120]\tLoss=0.1670\tacc=0.9571\t1580.1 examples/second\n","[Step=7136]\tLoss=0.1626\tacc=0.9591\t2344.4 examples/second\n","[Step=7152]\tLoss=0.1595\tacc=0.9603\t2494.1 examples/second\n","[Step=7168]\tLoss=0.1631\tacc=0.9588\t2489.1 examples/second\n","[Step=7184]\tLoss=0.1642\tacc=0.9581\t2455.6 examples/second\n","[Step=7200]\tLoss=0.1637\tacc=0.9580\t2278.2 examples/second\n","[Step=7216]\tLoss=0.1633\tacc=0.9580\t2482.7 examples/second\n","[Step=7232]\tLoss=0.1639\tacc=0.9576\t2441.5 examples/second\n","[Step=7248]\tLoss=0.1639\tacc=0.9571\t2468.3 examples/second\n","[Step=7264]\tLoss=0.1636\tacc=0.9574\t2488.9 examples/second\n","[Step=7280]\tLoss=0.1628\tacc=0.9578\t2516.4 examples/second\n","[Step=7296]\tLoss=0.1621\tacc=0.9582\t2466.7 examples/second\n","[Step=7312]\tLoss=0.1617\tacc=0.9580\t2491.6 examples/second\n","[Step=7328]\tLoss=0.1613\tacc=0.9581\t1628.3 examples/second\n","[Step=7344]\tLoss=0.1604\tacc=0.9584\t1477.4 examples/second\n","[Step=7360]\tLoss=0.1606\tacc=0.9583\t1510.6 examples/second\n","[Step=7376]\tLoss=0.1603\tacc=0.9584\t1478.4 examples/second\n","[Step=7392]\tLoss=0.1611\tacc=0.9580\t1710.3 examples/second\n","[Step=7408]\tLoss=0.1603\tacc=0.9582\t3299.4 examples/second\n","[Step=7424]\tLoss=0.1597\tacc=0.9585\t5668.8 examples/second\n","Test Loss=0.3023, Test acc=0.9088\n","\n","Epoch: 19\n","[Step=7440]\tLoss=0.1566\tacc=0.9609\t362.9 examples/second\n","[Step=7456]\tLoss=0.1488\tacc=0.9641\t2103.5 examples/second\n","[Step=7472]\tLoss=0.1622\tacc=0.9595\t2454.9 examples/second\n","[Step=7488]\tLoss=0.1584\tacc=0.9601\t2312.9 examples/second\n","[Step=7504]\tLoss=0.1587\tacc=0.9605\t1551.6 examples/second\n","[Step=7520]\tLoss=0.1607\tacc=0.9597\t1366.4 examples/second\n","[Step=7536]\tLoss=0.1606\tacc=0.9598\t1649.7 examples/second\n","[Step=7552]\tLoss=0.1579\tacc=0.9607\t1519.9 examples/second\n","[Step=7568]\tLoss=0.1589\tacc=0.9603\t1954.9 examples/second\n","[Step=7584]\tLoss=0.1570\tacc=0.9608\t2531.9 examples/second\n","[Step=7600]\tLoss=0.1577\tacc=0.9607\t2316.4 examples/second\n","[Step=7616]\tLoss=0.1583\tacc=0.9599\t2464.5 examples/second\n","[Step=7632]\tLoss=0.1588\tacc=0.9599\t2520.7 examples/second\n","[Step=7648]\tLoss=0.1581\tacc=0.9602\t2569.1 examples/second\n","[Step=7664]\tLoss=0.1573\tacc=0.9605\t2391.0 examples/second\n","[Step=7680]\tLoss=0.1572\tacc=0.9607\t2441.3 examples/second\n","[Step=7696]\tLoss=0.1582\tacc=0.9604\t2413.2 examples/second\n","[Step=7712]\tLoss=0.1585\tacc=0.9602\t2427.1 examples/second\n","[Step=7728]\tLoss=0.1595\tacc=0.9598\t2303.8 examples/second\n","[Step=7744]\tLoss=0.1592\tacc=0.9600\t2413.1 examples/second\n","[Step=7760]\tLoss=0.1587\tacc=0.9601\t1841.6 examples/second\n","[Step=7776]\tLoss=0.1595\tacc=0.9598\t1504.0 examples/second\n","[Step=7792]\tLoss=0.1594\tacc=0.9598\t1655.6 examples/second\n","[Step=7808]\tLoss=0.1603\tacc=0.9595\t5240.3 examples/second\n","Test Loss=0.3038, Test acc=0.9082\n","\n","Epoch: 20\n","[Step=7824]\tLoss=0.1808\tacc=0.9414\t357.2 examples/second\n","[Step=7840]\tLoss=0.1818\tacc=0.9508\t1443.2 examples/second\n","[Step=7856]\tLoss=0.1641\tacc=0.9581\t2535.8 examples/second\n","[Step=7872]\tLoss=0.1674\tacc=0.9567\t2452.4 examples/second\n","[Step=7888]\tLoss=0.1675\tacc=0.9560\t2510.1 examples/second\n","[Step=7904]\tLoss=0.1655\tacc=0.9562\t2503.8 examples/second\n","[Step=7920]\tLoss=0.1677\tacc=0.9548\t2367.5 examples/second\n","[Step=7936]\tLoss=0.1660\tacc=0.9551\t2258.1 examples/second\n","[Step=7952]\tLoss=0.1647\tacc=0.9552\t1494.0 examples/second\n","[Step=7968]\tLoss=0.1639\tacc=0.9558\t1533.0 examples/second\n","[Step=7984]\tLoss=0.1624\tacc=0.9560\t1698.9 examples/second\n","[Step=8000]\tLoss=0.1627\tacc=0.9560\t1432.2 examples/second\n","[Step=8016]\tLoss=0.1634\tacc=0.9561\t1976.4 examples/second\n","[Step=8032]\tLoss=0.1636\tacc=0.9562\t2519.7 examples/second\n","[Step=8048]\tLoss=0.1621\tacc=0.9568\t2517.0 examples/second\n","[Step=8064]\tLoss=0.1622\tacc=0.9568\t2460.9 examples/second\n","[Step=8080]\tLoss=0.1624\tacc=0.9567\t2387.7 examples/second\n","[Step=8096]\tLoss=0.1628\tacc=0.9568\t2464.1 examples/second\n","[Step=8112]\tLoss=0.1618\tacc=0.9572\t2409.3 examples/second\n","[Step=8128]\tLoss=0.1613\tacc=0.9574\t2565.2 examples/second\n","[Step=8144]\tLoss=0.1613\tacc=0.9576\t2406.9 examples/second\n","[Step=8160]\tLoss=0.1617\tacc=0.9574\t2512.4 examples/second\n","[Step=8176]\tLoss=0.1612\tacc=0.9575\t2469.1 examples/second\n","[Step=8192]\tLoss=0.1621\tacc=0.9572\t4414.3 examples/second\n","[Step=8208]\tLoss=0.1618\tacc=0.9574\t5648.2 examples/second\n","Test Loss=0.3044, Test acc=0.9094\n","\n","Epoch: 21\n","[Step=8224]\tLoss=0.1509\tacc=0.9639\t274.2 examples/second\n","[Step=8240]\tLoss=0.1604\tacc=0.9626\t2331.9 examples/second\n","[Step=8256]\tLoss=0.1562\tacc=0.9628\t2354.9 examples/second\n","[Step=8272]\tLoss=0.1546\tacc=0.9613\t2461.9 examples/second\n","[Step=8288]\tLoss=0.1489\tacc=0.9634\t2333.0 examples/second\n","[Step=8304]\tLoss=0.1496\tacc=0.9630\t2566.2 examples/second\n","[Step=8320]\tLoss=0.1524\tacc=0.9615\t2424.8 examples/second\n","[Step=8336]\tLoss=0.1563\tacc=0.9602\t2427.1 examples/second\n","[Step=8352]\tLoss=0.1552\tacc=0.9603\t2356.6 examples/second\n","[Step=8368]\tLoss=0.1568\tacc=0.9598\t1497.4 examples/second\n","[Step=8384]\tLoss=0.1578\tacc=0.9598\t1587.4 examples/second\n","[Step=8400]\tLoss=0.1574\tacc=0.9598\t1554.4 examples/second\n","[Step=8416]\tLoss=0.1584\tacc=0.9593\t1408.0 examples/second\n","[Step=8432]\tLoss=0.1579\tacc=0.9596\t1917.4 examples/second\n","[Step=8448]\tLoss=0.1590\tacc=0.9592\t2426.0 examples/second\n","[Step=8464]\tLoss=0.1595\tacc=0.9593\t2432.2 examples/second\n","[Step=8480]\tLoss=0.1603\tacc=0.9592\t2396.0 examples/second\n","[Step=8496]\tLoss=0.1628\tacc=0.9583\t2407.4 examples/second\n","[Step=8512]\tLoss=0.1628\tacc=0.9582\t2525.2 examples/second\n","[Step=8528]\tLoss=0.1642\tacc=0.9574\t2262.8 examples/second\n","[Step=8544]\tLoss=0.1645\tacc=0.9572\t2456.2 examples/second\n","[Step=8560]\tLoss=0.1637\tacc=0.9575\t2380.5 examples/second\n","[Step=8576]\tLoss=0.1642\tacc=0.9573\t2943.9 examples/second\n","[Step=8592]\tLoss=0.1630\tacc=0.9576\t5649.5 examples/second\n","Test Loss=0.3040, Test acc=0.9096\n","\n","Epoch: 22\n","[Step=8608]\tLoss=0.1654\tacc=0.9544\t284.5 examples/second\n","[Step=8624]\tLoss=0.1685\tacc=0.9563\t1573.4 examples/second\n","[Step=8640]\tLoss=0.1636\tacc=0.9579\t2492.9 examples/second\n","[Step=8656]\tLoss=0.1647\tacc=0.9570\t2250.1 examples/second\n","[Step=8672]\tLoss=0.1639\tacc=0.9567\t2385.0 examples/second\n","[Step=8688]\tLoss=0.1606\tacc=0.9578\t2404.0 examples/second\n","[Step=8704]\tLoss=0.1613\tacc=0.9576\t2322.8 examples/second\n","[Step=8720]\tLoss=0.1607\tacc=0.9577\t2528.3 examples/second\n","[Step=8736]\tLoss=0.1609\tacc=0.9577\t2377.9 examples/second\n","[Step=8752]\tLoss=0.1594\tacc=0.9583\t2409.8 examples/second\n","[Step=8768]\tLoss=0.1600\tacc=0.9582\t2060.6 examples/second\n","[Step=8784]\tLoss=0.1625\tacc=0.9571\t1483.1 examples/second\n","[Step=8800]\tLoss=0.1610\tacc=0.9577\t1560.8 examples/second\n","[Step=8816]\tLoss=0.1613\tacc=0.9574\t1583.8 examples/second\n","[Step=8832]\tLoss=0.1606\tacc=0.9574\t1516.9 examples/second\n","[Step=8848]\tLoss=0.1604\tacc=0.9577\t2152.4 examples/second\n","[Step=8864]\tLoss=0.1599\tacc=0.9580\t2420.5 examples/second\n","[Step=8880]\tLoss=0.1608\tacc=0.9579\t2330.4 examples/second\n","[Step=8896]\tLoss=0.1611\tacc=0.9576\t2537.8 examples/second\n","[Step=8912]\tLoss=0.1610\tacc=0.9575\t2567.0 examples/second\n","[Step=8928]\tLoss=0.1611\tacc=0.9577\t2340.9 examples/second\n","[Step=8944]\tLoss=0.1607\tacc=0.9578\t2491.6 examples/second\n","[Step=8960]\tLoss=0.1603\tacc=0.9578\t2371.0 examples/second\n","[Step=8976]\tLoss=0.1607\tacc=0.9577\t4817.2 examples/second\n","[Step=8992]\tLoss=0.1609\tacc=0.9574\t5584.9 examples/second\n","Test Loss=0.3013, Test acc=0.9103\n","\n","Epoch: 23\n","[Step=9008]\tLoss=0.1795\tacc=0.9516\t265.4 examples/second\n","[Step=9024]\tLoss=0.1751\tacc=0.9529\t1876.5 examples/second\n","[Step=9040]\tLoss=0.1730\tacc=0.9533\t2334.8 examples/second\n","[Step=9056]\tLoss=0.1690\tacc=0.9547\t2291.5 examples/second\n","[Step=9072]\tLoss=0.1725\tacc=0.9538\t2415.2 examples/second\n","[Step=9088]\tLoss=0.1703\tacc=0.9551\t2450.6 examples/second\n","[Step=9104]\tLoss=0.1705\tacc=0.9550\t2451.4 examples/second\n","[Step=9120]\tLoss=0.1678\tacc=0.9558\t2608.0 examples/second\n","[Step=9136]\tLoss=0.1663\tacc=0.9565\t2516.3 examples/second\n","[Step=9152]\tLoss=0.1654\tacc=0.9564\t2416.0 examples/second\n","[Step=9168]\tLoss=0.1647\tacc=0.9566\t2482.9 examples/second\n","[Step=9184]\tLoss=0.1638\tacc=0.9569\t1562.2 examples/second\n","[Step=9200]\tLoss=0.1644\tacc=0.9568\t1487.6 examples/second\n","[Step=9216]\tLoss=0.1644\tacc=0.9566\t1475.3 examples/second\n","[Step=9232]\tLoss=0.1644\tacc=0.9568\t1721.1 examples/second\n","[Step=9248]\tLoss=0.1637\tacc=0.9571\t1637.6 examples/second\n","[Step=9264]\tLoss=0.1632\tacc=0.9574\t2320.9 examples/second\n","[Step=9280]\tLoss=0.1639\tacc=0.9570\t1439.0 examples/second\n","[Step=9296]\tLoss=0.1632\tacc=0.9574\t1509.6 examples/second\n","[Step=9312]\tLoss=0.1630\tacc=0.9574\t1679.1 examples/second\n","[Step=9328]\tLoss=0.1620\tacc=0.9576\t1549.9 examples/second\n","[Step=9344]\tLoss=0.1623\tacc=0.9574\t1839.0 examples/second\n","[Step=9360]\tLoss=0.1624\tacc=0.9574\t3557.3 examples/second\n","[Step=9376]\tLoss=0.1617\tacc=0.9578\t5700.4 examples/second\n","Test Loss=0.3040, Test acc=0.9100\n","\n","Epoch: 24\n","[Step=9392]\tLoss=0.1525\tacc=0.9629\t243.5 examples/second\n","[Step=9408]\tLoss=0.1654\tacc=0.9577\t1092.5 examples/second\n","[Step=9424]\tLoss=0.1544\tacc=0.9602\t1860.3 examples/second\n","[Step=9440]\tLoss=0.1568\tacc=0.9577\t2324.9 examples/second\n","[Step=9456]\tLoss=0.1594\tacc=0.9569\t2236.1 examples/second\n","[Step=9472]\tLoss=0.1598\tacc=0.9566\t2501.0 examples/second\n","[Step=9488]\tLoss=0.1598\tacc=0.9570\t2430.4 examples/second\n","[Step=9504]\tLoss=0.1620\tacc=0.9564\t2602.5 examples/second\n","[Step=9520]\tLoss=0.1615\tacc=0.9565\t2431.5 examples/second\n","[Step=9536]\tLoss=0.1596\tacc=0.9574\t1677.3 examples/second\n","[Step=9552]\tLoss=0.1597\tacc=0.9577\t1441.1 examples/second\n","[Step=9568]\tLoss=0.1606\tacc=0.9572\t1495.8 examples/second\n","[Step=9584]\tLoss=0.1597\tacc=0.9575\t1518.0 examples/second\n","[Step=9600]\tLoss=0.1602\tacc=0.9576\t1855.2 examples/second\n","[Step=9616]\tLoss=0.1608\tacc=0.9574\t2451.8 examples/second\n","[Step=9632]\tLoss=0.1619\tacc=0.9575\t2556.9 examples/second\n","[Step=9648]\tLoss=0.1622\tacc=0.9574\t2327.9 examples/second\n","[Step=9664]\tLoss=0.1613\tacc=0.9578\t2548.7 examples/second\n","[Step=9680]\tLoss=0.1609\tacc=0.9579\t2363.6 examples/second\n","[Step=9696]\tLoss=0.1599\tacc=0.9581\t2479.4 examples/second\n","[Step=9712]\tLoss=0.1601\tacc=0.9580\t2544.4 examples/second\n","[Step=9728]\tLoss=0.1605\tacc=0.9579\t2327.3 examples/second\n","[Step=9744]\tLoss=0.1610\tacc=0.9578\t2538.7 examples/second\n","[Step=9760]\tLoss=0.1609\tacc=0.9577\t5413.6 examples/second\n","Test Loss=0.3019, Test acc=0.9097\n","\n","Epoch: 25\n","[Step=9776]\tLoss=0.0860\tacc=0.9766\t315.0 examples/second\n","[Step=9792]\tLoss=0.1613\tacc=0.9554\t1494.7 examples/second\n","[Step=9808]\tLoss=0.1648\tacc=0.9548\t2432.5 examples/second\n","[Step=9824]\tLoss=0.1674\tacc=0.9550\t1820.6 examples/second\n","[Step=9840]\tLoss=0.1645\tacc=0.9565\t2522.2 examples/second\n","[Step=9856]\tLoss=0.1623\tacc=0.9566\t2579.4 examples/second\n","[Step=9872]\tLoss=0.1635\tacc=0.9563\t2309.3 examples/second\n","[Step=9888]\tLoss=0.1662\tacc=0.9557\t2458.4 examples/second\n","[Step=9904]\tLoss=0.1666\tacc=0.9559\t2405.7 examples/second\n","[Step=9920]\tLoss=0.1654\tacc=0.9563\t2473.9 examples/second\n","[Step=9936]\tLoss=0.1660\tacc=0.9556\t2563.2 examples/second\n","[Step=9952]\tLoss=0.1635\tacc=0.9567\t1506.5 examples/second\n","[Step=9968]\tLoss=0.1619\tacc=0.9573\t1471.0 examples/second\n","[Step=9984]\tLoss=0.1610\tacc=0.9574\t1524.0 examples/second\n","[Step=10000]\tLoss=0.1622\tacc=0.9570\t1604.0 examples/second\n","[Step=10016]\tLoss=0.1630\tacc=0.9567\t1957.8 examples/second\n","[Step=10032]\tLoss=0.1628\tacc=0.9570\t2458.8 examples/second\n","[Step=10048]\tLoss=0.1619\tacc=0.9572\t2406.4 examples/second\n","[Step=10064]\tLoss=0.1617\tacc=0.9573\t2401.3 examples/second\n","[Step=10080]\tLoss=0.1627\tacc=0.9569\t2378.6 examples/second\n","[Step=10096]\tLoss=0.1620\tacc=0.9573\t2663.1 examples/second\n","[Step=10112]\tLoss=0.1628\tacc=0.9571\t2288.6 examples/second\n","[Step=10128]\tLoss=0.1629\tacc=0.9570\t2520.7 examples/second\n","[Step=10144]\tLoss=0.1629\tacc=0.9570\t3459.9 examples/second\n","[Step=10160]\tLoss=0.1632\tacc=0.9568\t5648.3 examples/second\n","Test Loss=0.3034, Test acc=0.9088\n","\n","Epoch: 26\n","[Step=10176]\tLoss=0.1798\tacc=0.9508\t282.1 examples/second\n","[Step=10192]\tLoss=0.1715\tacc=0.9534\t2108.9 examples/second\n","[Step=10208]\tLoss=0.1640\tacc=0.9561\t2581.7 examples/second\n","[Step=10224]\tLoss=0.1643\tacc=0.9562\t2366.6 examples/second\n","[Step=10240]\tLoss=0.1688\tacc=0.9554\t2516.8 examples/second\n","[Step=10256]\tLoss=0.1705\tacc=0.9541\t1754.4 examples/second\n","[Step=10272]\tLoss=0.1692\tacc=0.9545\t2342.9 examples/second\n","[Step=10288]\tLoss=0.1680\tacc=0.9549\t2474.3 examples/second\n","[Step=10304]\tLoss=0.1679\tacc=0.9551\t2174.4 examples/second\n","[Step=10320]\tLoss=0.1668\tacc=0.9554\t2179.7 examples/second\n","[Step=10336]\tLoss=0.1651\tacc=0.9557\t1980.8 examples/second\n","[Step=10352]\tLoss=0.1649\tacc=0.9558\t1467.7 examples/second\n","[Step=10368]\tLoss=0.1644\tacc=0.9561\t1110.1 examples/second\n","[Step=10384]\tLoss=0.1634\tacc=0.9565\t1222.5 examples/second\n","[Step=10400]\tLoss=0.1631\tacc=0.9567\t1522.5 examples/second\n","[Step=10416]\tLoss=0.1635\tacc=0.9568\t2195.9 examples/second\n","[Step=10432]\tLoss=0.1635\tacc=0.9567\t2414.0 examples/second\n","[Step=10448]\tLoss=0.1637\tacc=0.9567\t2412.5 examples/second\n","[Step=10464]\tLoss=0.1619\tacc=0.9574\t2460.1 examples/second\n","[Step=10480]\tLoss=0.1621\tacc=0.9574\t2524.9 examples/second\n","[Step=10496]\tLoss=0.1624\tacc=0.9574\t2513.3 examples/second\n","[Step=10512]\tLoss=0.1623\tacc=0.9576\t2347.7 examples/second\n","[Step=10528]\tLoss=0.1623\tacc=0.9576\t2707.5 examples/second\n","[Step=10544]\tLoss=0.1618\tacc=0.9578\t5685.0 examples/second\n","Test Loss=0.3037, Test acc=0.9098\n","\n","Epoch: 27\n","[Step=10560]\tLoss=0.1496\tacc=0.9557\t305.3 examples/second\n","[Step=10576]\tLoss=0.1527\tacc=0.9568\t1599.9 examples/second\n","[Step=10592]\tLoss=0.1530\tacc=0.9580\t2571.8 examples/second\n","[Step=10608]\tLoss=0.1566\tacc=0.9576\t2382.8 examples/second\n","[Step=10624]\tLoss=0.1593\tacc=0.9567\t2300.5 examples/second\n","[Step=10640]\tLoss=0.1596\tacc=0.9570\t2451.6 examples/second\n","[Step=10656]\tLoss=0.1584\tacc=0.9580\t2226.1 examples/second\n","[Step=10672]\tLoss=0.1600\tacc=0.9573\t1833.6 examples/second\n","[Step=10688]\tLoss=0.1616\tacc=0.9568\t2444.4 examples/second\n","[Step=10704]\tLoss=0.1610\tacc=0.9571\t2393.9 examples/second\n","[Step=10720]\tLoss=0.1604\tacc=0.9574\t2483.5 examples/second\n","[Step=10736]\tLoss=0.1613\tacc=0.9570\t2228.9 examples/second\n","[Step=10752]\tLoss=0.1605\tacc=0.9575\t1502.5 examples/second\n","[Step=10768]\tLoss=0.1604\tacc=0.9576\t1496.5 examples/second\n","[Step=10784]\tLoss=0.1604\tacc=0.9576\t1576.5 examples/second\n","[Step=10800]\tLoss=0.1606\tacc=0.9577\t1514.5 examples/second\n","[Step=10816]\tLoss=0.1611\tacc=0.9578\t1799.4 examples/second\n","[Step=10832]\tLoss=0.1605\tacc=0.9580\t2263.8 examples/second\n","[Step=10848]\tLoss=0.1603\tacc=0.9581\t2357.9 examples/second\n","[Step=10864]\tLoss=0.1598\tacc=0.9584\t2375.2 examples/second\n","[Step=10880]\tLoss=0.1601\tacc=0.9580\t2518.8 examples/second\n","[Step=10896]\tLoss=0.1605\tacc=0.9577\t2510.9 examples/second\n","[Step=10912]\tLoss=0.1602\tacc=0.9581\t2501.0 examples/second\n","[Step=10928]\tLoss=0.1605\tacc=0.9581\t3609.1 examples/second\n","[Step=10944]\tLoss=0.1603\tacc=0.9583\t5652.5 examples/second\n","Test Loss=0.3030, Test acc=0.9082\n","\n","Epoch: 28\n","[Step=10960]\tLoss=0.1714\tacc=0.9564\t288.5 examples/second\n","[Step=10976]\tLoss=0.1675\tacc=0.9579\t1422.0 examples/second\n","[Step=10992]\tLoss=0.1604\tacc=0.9597\t1671.8 examples/second\n","[Step=11008]\tLoss=0.1623\tacc=0.9583\t2493.6 examples/second\n","[Step=11024]\tLoss=0.1598\tacc=0.9598\t2469.7 examples/second\n","[Step=11040]\tLoss=0.1577\tacc=0.9608\t2450.4 examples/second\n","[Step=11056]\tLoss=0.1573\tacc=0.9608\t2376.2 examples/second\n","[Step=11072]\tLoss=0.1573\tacc=0.9606\t2599.6 examples/second\n","[Step=11088]\tLoss=0.1598\tacc=0.9590\t1830.6 examples/second\n","[Step=11104]\tLoss=0.1580\tacc=0.9597\t2275.8 examples/second\n","[Step=11120]\tLoss=0.1583\tacc=0.9595\t2469.0 examples/second\n","[Step=11136]\tLoss=0.1582\tacc=0.9596\t2533.7 examples/second\n","[Step=11152]\tLoss=0.1570\tacc=0.9601\t2395.6 examples/second\n","[Step=11168]\tLoss=0.1571\tacc=0.9600\t2427.2 examples/second\n","[Step=11184]\tLoss=0.1588\tacc=0.9595\t1738.6 examples/second\n","[Step=11200]\tLoss=0.1605\tacc=0.9589\t1515.6 examples/second\n","[Step=11216]\tLoss=0.1610\tacc=0.9586\t1510.5 examples/second\n","[Step=11232]\tLoss=0.1600\tacc=0.9591\t1585.6 examples/second\n","[Step=11248]\tLoss=0.1603\tacc=0.9589\t1649.2 examples/second\n","[Step=11264]\tLoss=0.1597\tacc=0.9590\t2314.0 examples/second\n","[Step=11280]\tLoss=0.1593\tacc=0.9590\t2481.2 examples/second\n","[Step=11296]\tLoss=0.1600\tacc=0.9586\t2383.8 examples/second\n","[Step=11312]\tLoss=0.1603\tacc=0.9584\t2851.2 examples/second\n","[Step=11328]\tLoss=0.1603\tacc=0.9583\t5721.2 examples/second\n","Test Loss=0.3011, Test acc=0.9096\n","\n","Epoch: 29\n","[Step=11344]\tLoss=0.1443\tacc=0.9641\t392.3 examples/second\n","[Step=11360]\tLoss=0.1641\tacc=0.9568\t1531.2 examples/second\n","[Step=11376]\tLoss=0.1536\tacc=0.9607\t1433.7 examples/second\n","[Step=11392]\tLoss=0.1539\tacc=0.9612\t1494.9 examples/second\n","[Step=11408]\tLoss=0.1598\tacc=0.9590\t1597.1 examples/second\n","[Step=11424]\tLoss=0.1596\tacc=0.9594\t1475.5 examples/second\n","[Step=11440]\tLoss=0.1606\tacc=0.9585\t2326.9 examples/second\n","[Step=11456]\tLoss=0.1613\tacc=0.9581\t2370.7 examples/second\n","[Step=11472]\tLoss=0.1600\tacc=0.9581\t2467.9 examples/second\n","[Step=11488]\tLoss=0.1597\tacc=0.9584\t2500.7 examples/second\n","[Step=11504]\tLoss=0.1569\tacc=0.9593\t1884.3 examples/second\n","[Step=11520]\tLoss=0.1584\tacc=0.9590\t2367.6 examples/second\n","[Step=11536]\tLoss=0.1591\tacc=0.9586\t2525.1 examples/second\n","[Step=11552]\tLoss=0.1593\tacc=0.9582\t2446.8 examples/second\n","[Step=11568]\tLoss=0.1592\tacc=0.9581\t2537.6 examples/second\n","[Step=11584]\tLoss=0.1604\tacc=0.9581\t2556.2 examples/second\n","[Step=11600]\tLoss=0.1602\tacc=0.9583\t2415.3 examples/second\n","[Step=11616]\tLoss=0.1603\tacc=0.9579\t2135.0 examples/second\n","[Step=11632]\tLoss=0.1607\tacc=0.9577\t1480.7 examples/second\n","[Step=11648]\tLoss=0.1600\tacc=0.9580\t1398.2 examples/second\n","[Step=11664]\tLoss=0.1602\tacc=0.9581\t1281.3 examples/second\n","[Step=11680]\tLoss=0.1600\tacc=0.9581\t1335.2 examples/second\n","[Step=11696]\tLoss=0.1604\tacc=0.9579\t1291.3 examples/second\n","[Step=11712]\tLoss=0.1601\tacc=0.9581\t2766.5 examples/second\n","[Step=11728]\tLoss=0.1605\tacc=0.9581\t5588.2 examples/second\n","Test Loss=0.3010, Test acc=0.9087\n"]}],"source":["# Uncomment to load pretrained weights\n","# net.load_state_dict(torch.load(\"net_after_pruning.pt\"))\n","# Comment if you have loaded pretrained weights\n","finetune_after_prune(net, epochs=30, batch_size=128, lr=0.001, reg=0.00001)\n"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"bvACtNF6yGVF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698444155120,"user_tz":240,"elapsed":5390,"user":{"displayName":"Amarnath Shinde","userId":"15881715752542230515"}},"outputId":"6d4323a4-ff1f-4502-abe4-52efe08d3271"},"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","Test Loss=0.3022, Test accuracy=0.9107\n"]},{"output_type":"execute_result","data":{"text/plain":["0.9107"]},"metadata":{},"execution_count":13}],"source":["# Load the best weight paramters\n","net.load_state_dict(torch.load(\"net_after_pruning.pt\"))\n","test(net)"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jaWRpi8dyGVG","executionInfo":{"status":"ok","timestamp":1698444158552,"user_tz":240,"elapsed":340,"user":{"displayName":"Amarnath Shinde","userId":"15881715752542230515"}},"outputId":"f62faf4b-ee12-4992-f875-877e619615f3"},"outputs":[{"output_type":"stream","name":"stdout","text":["-----Summary After pruning-----\n","Layer id\tType\t\tParameter\tNon-zero parameter\tSparsity(\\%)\n","1\t\tConvolutional\t864\t\t387\t\t\t0.552083\n","2\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","3\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","4\t\tConvolutional\t9216\t\t4071\t\t\t0.558268\n","5\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","6\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","7\t\tConvolutional\t18432\t\t10742\t\t\t0.417209\n","8\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","9\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","10\t\tConvolutional\t36864\t\t22330\t\t\t0.394260\n","11\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","12\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","13\t\tConvolutional\t73728\t\t44499\t\t\t0.396444\n","14\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","15\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","16\t\tConvolutional\t147456\t\t90448\t\t\t0.386610\n","17\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","18\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","19\t\tConvolutional\t147456\t\t88988\t\t\t0.396512\n","20\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","21\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","22\t\tConvolutional\t294912\t\t173091\t\t\t0.413076\n","23\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","24\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","25\t\tConvolutional\t589824\t\t335014\t\t\t0.432010\n","26\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","27\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","28\t\tConvolutional\t589824\t\t319733\t\t\t0.457918\n","29\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","30\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","31\t\tConvolutional\t589824\t\t279365\t\t\t0.526359\n","32\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","33\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","34\t\tConvolutional\t589824\t\t239562\t\t\t0.593842\n","35\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","36\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","37\t\tConvolutional\t589824\t\t183037\t\t\t0.689675\n","38\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","39\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","40\t\tLinear\t\t65536\t\t38545\t\t\t0.411850\n","41\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","42\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","43\t\tLinear\t\t65536\t\t33573\t\t\t0.487717\n","44\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","45\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","46\t\tLinear\t\t2560\t\t966\t\t\t0.622656\n","Total nonzero parameters: 1864351\n","Total parameters: 3811680\n","Total sparsity: 0.510885\n","-------------------------------\n"]}],"source":["print(\"-----Summary After pruning-----\")\n","summary(net)\n","print(\"-------------------------------\")"]},{"cell_type":"code","source":["# Test accuracy before fine-tuning\n","prune(net, method='std', q=45.0, s=0.75)\n","test(net)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FV27NXgXj2XZ","executionInfo":{"status":"ok","timestamp":1698444173719,"user_tz":240,"elapsed":3955,"user":{"displayName":"Amarnath Shinde","userId":"15881715752542230515"}},"outputId":"802c11b3-276b-43c5-d943-fc26370fafde"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","Test Loss=0.4453, Test accuracy=0.8740\n"]},{"output_type":"execute_result","data":{"text/plain":["0.874"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["# Uncomment to load pretrained weights\n","# net.load_state_dict(torch.load(\"net_after_pruning.pt\"))\n","# Comment if you have loaded pretrained weights\n","finetune_after_prune(net, epochs=30, batch_size=128, lr=0.001, reg=0.00001)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uLPzTAhckKZ6","executionInfo":{"status":"ok","timestamp":1698445103445,"user_tz":240,"elapsed":906529,"user":{"displayName":"Amarnath Shinde","userId":"15881715752542230515"}},"outputId":"b5a8e979-1770-4adf-bf65-b716a9d3043d"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["==> Preparing data..\n","Files already downloaded and verified\n","Files already downloaded and verified\n","\n","Epoch: 0\n","[Step=16]\tLoss=0.2427\tacc=0.9375\t513.2 examples/second\n","[Step=32]\tLoss=0.2360\tacc=0.9385\t1568.4 examples/second\n","[Step=48]\tLoss=0.2350\tacc=0.9395\t1488.7 examples/second\n","[Step=64]\tLoss=0.2342\tacc=0.9386\t2177.0 examples/second\n","[Step=80]\tLoss=0.2357\tacc=0.9380\t2369.8 examples/second\n","[Step=96]\tLoss=0.2338\tacc=0.9388\t2230.4 examples/second\n","[Step=112]\tLoss=0.2334\tacc=0.9385\t2023.7 examples/second\n","[Step=128]\tLoss=0.2314\tacc=0.9392\t2465.4 examples/second\n","[Step=144]\tLoss=0.2307\tacc=0.9387\t2467.9 examples/second\n","[Step=160]\tLoss=0.2311\tacc=0.9380\t2398.8 examples/second\n","[Step=176]\tLoss=0.2314\tacc=0.9379\t2412.9 examples/second\n","[Step=192]\tLoss=0.2297\tacc=0.9386\t2426.6 examples/second\n","[Step=208]\tLoss=0.2295\tacc=0.9391\t2518.4 examples/second\n","[Step=224]\tLoss=0.2296\tacc=0.9388\t2403.6 examples/second\n","[Step=240]\tLoss=0.2312\tacc=0.9382\t2166.3 examples/second\n","[Step=256]\tLoss=0.2305\tacc=0.9382\t1485.4 examples/second\n","[Step=272]\tLoss=0.2311\tacc=0.9376\t1452.5 examples/second\n","[Step=288]\tLoss=0.2304\tacc=0.9377\t1540.1 examples/second\n","[Step=304]\tLoss=0.2292\tacc=0.9382\t1560.9 examples/second\n","[Step=320]\tLoss=0.2298\tacc=0.9380\t2423.5 examples/second\n","[Step=336]\tLoss=0.2295\tacc=0.9381\t2483.8 examples/second\n","[Step=352]\tLoss=0.2299\tacc=0.9380\t2298.2 examples/second\n","[Step=368]\tLoss=0.2293\tacc=0.9383\t3393.4 examples/second\n","[Step=384]\tLoss=0.2294\tacc=0.9382\t5619.2 examples/second\n","Test Loss=0.3639, Test acc=0.8942\n","Saving...\n","\n","Epoch: 1\n","[Step=400]\tLoss=0.2585\tacc=0.9262\t366.1 examples/second\n","[Step=416]\tLoss=0.2224\tacc=0.9378\t1930.4 examples/second\n","[Step=432]\tLoss=0.2231\tacc=0.9392\t1459.7 examples/second\n","[Step=448]\tLoss=0.2322\tacc=0.9371\t1344.9 examples/second\n","[Step=464]\tLoss=0.2314\tacc=0.9377\t1565.0 examples/second\n","[Step=480]\tLoss=0.2291\tacc=0.9385\t1492.5 examples/second\n","[Step=496]\tLoss=0.2305\tacc=0.9371\t2199.0 examples/second\n","[Step=512]\tLoss=0.2291\tacc=0.9377\t2302.8 examples/second\n","[Step=528]\tLoss=0.2293\tacc=0.9379\t1821.6 examples/second\n","[Step=544]\tLoss=0.2278\tacc=0.9385\t2531.8 examples/second\n","[Step=560]\tLoss=0.2289\tacc=0.9385\t2455.5 examples/second\n","[Step=576]\tLoss=0.2276\tacc=0.9389\t2485.2 examples/second\n","[Step=592]\tLoss=0.2261\tacc=0.9397\t2413.4 examples/second\n","[Step=608]\tLoss=0.2259\tacc=0.9399\t2530.5 examples/second\n","[Step=624]\tLoss=0.2261\tacc=0.9399\t2348.7 examples/second\n","[Step=640]\tLoss=0.2255\tacc=0.9401\t2371.6 examples/second\n","[Step=656]\tLoss=0.2268\tacc=0.9398\t2406.3 examples/second\n","[Step=672]\tLoss=0.2253\tacc=0.9404\t2031.0 examples/second\n","[Step=688]\tLoss=0.2266\tacc=0.9396\t1535.7 examples/second\n","[Step=704]\tLoss=0.2280\tacc=0.9391\t1513.8 examples/second\n","[Step=720]\tLoss=0.2290\tacc=0.9387\t1501.8 examples/second\n","[Step=736]\tLoss=0.2296\tacc=0.9382\t1522.1 examples/second\n","[Step=752]\tLoss=0.2284\tacc=0.9387\t2623.9 examples/second\n","[Step=768]\tLoss=0.2286\tacc=0.9387\t5638.0 examples/second\n","Test Loss=0.3630, Test acc=0.8947\n","Saving...\n","\n","Epoch: 2\n","[Step=784]\tLoss=0.2015\tacc=0.9531\t401.7 examples/second\n","[Step=800]\tLoss=0.2283\tacc=0.9379\t1509.3 examples/second\n","[Step=816]\tLoss=0.2409\tacc=0.9347\t2333.5 examples/second\n","[Step=832]\tLoss=0.2390\tacc=0.9356\t2387.5 examples/second\n","[Step=848]\tLoss=0.2424\tacc=0.9348\t1833.8 examples/second\n","[Step=864]\tLoss=0.2374\tacc=0.9365\t1487.9 examples/second\n","[Step=880]\tLoss=0.2397\tacc=0.9362\t1522.9 examples/second\n","[Step=896]\tLoss=0.2405\tacc=0.9359\t1555.1 examples/second\n","[Step=912]\tLoss=0.2397\tacc=0.9362\t1631.3 examples/second\n","[Step=928]\tLoss=0.2383\tacc=0.9366\t2275.6 examples/second\n","[Step=944]\tLoss=0.2387\tacc=0.9369\t1787.2 examples/second\n","[Step=960]\tLoss=0.2383\tacc=0.9364\t2485.5 examples/second\n","[Step=976]\tLoss=0.2378\tacc=0.9363\t2536.5 examples/second\n","[Step=992]\tLoss=0.2367\tacc=0.9365\t2230.6 examples/second\n","[Step=1008]\tLoss=0.2370\tacc=0.9362\t1779.6 examples/second\n","[Step=1024]\tLoss=0.2389\tacc=0.9358\t1267.5 examples/second\n","[Step=1040]\tLoss=0.2386\tacc=0.9357\t965.1 examples/second\n","[Step=1056]\tLoss=0.2370\tacc=0.9363\t1381.2 examples/second\n","[Step=1072]\tLoss=0.2360\tacc=0.9366\t1250.4 examples/second\n","[Step=1088]\tLoss=0.2355\tacc=0.9367\t1306.3 examples/second\n","[Step=1104]\tLoss=0.2340\tacc=0.9370\t1557.6 examples/second\n","[Step=1120]\tLoss=0.2339\tacc=0.9369\t1591.7 examples/second\n","[Step=1136]\tLoss=0.2337\tacc=0.9369\t1378.5 examples/second\n","[Step=1152]\tLoss=0.2331\tacc=0.9374\t2249.4 examples/second\n","[Step=1168]\tLoss=0.2323\tacc=0.9378\t5569.2 examples/second\n","Test Loss=0.3629, Test acc=0.8959\n","Saving...\n","\n","Epoch: 3\n","[Step=1184]\tLoss=0.1992\tacc=0.9432\t299.2 examples/second\n","[Step=1200]\tLoss=0.2293\tacc=0.9349\t2368.6 examples/second\n","[Step=1216]\tLoss=0.2315\tacc=0.9359\t2404.8 examples/second\n","[Step=1232]\tLoss=0.2308\tacc=0.9360\t1376.7 examples/second\n","[Step=1248]\tLoss=0.2267\tacc=0.9389\t1523.7 examples/second\n","[Step=1264]\tLoss=0.2269\tacc=0.9390\t1474.9 examples/second\n","[Step=1280]\tLoss=0.2325\tacc=0.9370\t1309.1 examples/second\n","[Step=1296]\tLoss=0.2325\tacc=0.9378\t1486.5 examples/second\n","[Step=1312]\tLoss=0.2284\tacc=0.9395\t2195.8 examples/second\n","[Step=1328]\tLoss=0.2269\tacc=0.9397\t2423.0 examples/second\n","[Step=1344]\tLoss=0.2261\tacc=0.9396\t2468.4 examples/second\n","[Step=1360]\tLoss=0.2269\tacc=0.9396\t2286.8 examples/second\n","[Step=1376]\tLoss=0.2266\tacc=0.9396\t2443.9 examples/second\n","[Step=1392]\tLoss=0.2289\tacc=0.9387\t2422.2 examples/second\n","[Step=1408]\tLoss=0.2269\tacc=0.9393\t2328.5 examples/second\n","[Step=1424]\tLoss=0.2271\tacc=0.9390\t2313.4 examples/second\n","[Step=1440]\tLoss=0.2279\tacc=0.9388\t2336.7 examples/second\n","[Step=1456]\tLoss=0.2279\tacc=0.9390\t2436.3 examples/second\n","[Step=1472]\tLoss=0.2293\tacc=0.9386\t2441.6 examples/second\n","[Step=1488]\tLoss=0.2293\tacc=0.9385\t1990.3 examples/second\n","[Step=1504]\tLoss=0.2284\tacc=0.9389\t1406.6 examples/second\n","[Step=1520]\tLoss=0.2291\tacc=0.9389\t1507.4 examples/second\n","[Step=1536]\tLoss=0.2292\tacc=0.9388\t1759.5 examples/second\n","[Step=1552]\tLoss=0.2296\tacc=0.9386\t5237.6 examples/second\n","Test Loss=0.3646, Test acc=0.8941\n","\n","Epoch: 4\n","[Step=1568]\tLoss=0.2125\tacc=0.9434\t379.4 examples/second\n","[Step=1584]\tLoss=0.2370\tacc=0.9328\t1498.6 examples/second\n","[Step=1600]\tLoss=0.2420\tacc=0.9314\t2436.3 examples/second\n","[Step=1616]\tLoss=0.2373\tacc=0.9337\t2340.3 examples/second\n","[Step=1632]\tLoss=0.2394\tacc=0.9327\t2456.6 examples/second\n","[Step=1648]\tLoss=0.2401\tacc=0.9331\t2296.0 examples/second\n","[Step=1664]\tLoss=0.2403\tacc=0.9337\t2284.6 examples/second\n","[Step=1680]\tLoss=0.2384\tacc=0.9347\t1486.2 examples/second\n","[Step=1696]\tLoss=0.2386\tacc=0.9348\t1353.7 examples/second\n","[Step=1712]\tLoss=0.2352\tacc=0.9360\t1461.2 examples/second\n","[Step=1728]\tLoss=0.2346\tacc=0.9364\t1537.1 examples/second\n","[Step=1744]\tLoss=0.2326\tacc=0.9367\t1704.0 examples/second\n","[Step=1760]\tLoss=0.2332\tacc=0.9363\t2433.2 examples/second\n","[Step=1776]\tLoss=0.2327\tacc=0.9363\t2425.0 examples/second\n","[Step=1792]\tLoss=0.2332\tacc=0.9363\t2507.9 examples/second\n","[Step=1808]\tLoss=0.2314\tacc=0.9369\t2344.0 examples/second\n","[Step=1824]\tLoss=0.2299\tacc=0.9374\t2390.1 examples/second\n","[Step=1840]\tLoss=0.2304\tacc=0.9372\t2452.0 examples/second\n","[Step=1856]\tLoss=0.2296\tacc=0.9378\t2362.7 examples/second\n","[Step=1872]\tLoss=0.2302\tacc=0.9377\t2465.5 examples/second\n","[Step=1888]\tLoss=0.2311\tacc=0.9374\t2356.9 examples/second\n","[Step=1904]\tLoss=0.2303\tacc=0.9376\t2468.0 examples/second\n","[Step=1920]\tLoss=0.2301\tacc=0.9376\t2376.1 examples/second\n","[Step=1936]\tLoss=0.2310\tacc=0.9372\t4063.0 examples/second\n","[Step=1952]\tLoss=0.2300\tacc=0.9377\t5655.4 examples/second\n","Test Loss=0.3597, Test acc=0.8955\n","\n","Epoch: 5\n","[Step=1968]\tLoss=0.2429\tacc=0.9315\t280.3 examples/second\n","[Step=1984]\tLoss=0.2272\tacc=0.9370\t2210.1 examples/second\n","[Step=2000]\tLoss=0.2338\tacc=0.9373\t2420.7 examples/second\n","[Step=2016]\tLoss=0.2357\tacc=0.9357\t2425.9 examples/second\n","[Step=2032]\tLoss=0.2363\tacc=0.9365\t2358.2 examples/second\n","[Step=2048]\tLoss=0.2344\tacc=0.9374\t2376.7 examples/second\n","[Step=2064]\tLoss=0.2331\tacc=0.9382\t2700.7 examples/second\n","[Step=2080]\tLoss=0.2333\tacc=0.9386\t2223.1 examples/second\n","[Step=2096]\tLoss=0.2347\tacc=0.9374\t1574.7 examples/second\n","[Step=2112]\tLoss=0.2326\tacc=0.9378\t1421.0 examples/second\n","[Step=2128]\tLoss=0.2328\tacc=0.9377\t1567.3 examples/second\n","[Step=2144]\tLoss=0.2336\tacc=0.9372\t1365.1 examples/second\n","[Step=2160]\tLoss=0.2332\tacc=0.9373\t1607.5 examples/second\n","[Step=2176]\tLoss=0.2319\tacc=0.9378\t2510.3 examples/second\n","[Step=2192]\tLoss=0.2321\tacc=0.9377\t2379.0 examples/second\n","[Step=2208]\tLoss=0.2315\tacc=0.9377\t2463.6 examples/second\n","[Step=2224]\tLoss=0.2312\tacc=0.9379\t2441.3 examples/second\n","[Step=2240]\tLoss=0.2323\tacc=0.9375\t2398.2 examples/second\n","[Step=2256]\tLoss=0.2315\tacc=0.9379\t2429.4 examples/second\n","[Step=2272]\tLoss=0.2319\tacc=0.9375\t2492.6 examples/second\n","[Step=2288]\tLoss=0.2317\tacc=0.9376\t2398.3 examples/second\n","[Step=2304]\tLoss=0.2318\tacc=0.9375\t2293.5 examples/second\n","[Step=2320]\tLoss=0.2310\tacc=0.9379\t2793.6 examples/second\n","[Step=2336]\tLoss=0.2307\tacc=0.9380\t5670.0 examples/second\n","Test Loss=0.3608, Test acc=0.8950\n","\n","Epoch: 6\n","[Step=2352]\tLoss=0.2508\tacc=0.9284\t283.8 examples/second\n","[Step=2368]\tLoss=0.2419\tacc=0.9357\t1941.7 examples/second\n","[Step=2384]\tLoss=0.2369\tacc=0.9363\t2441.1 examples/second\n","[Step=2400]\tLoss=0.2348\tacc=0.9371\t2395.5 examples/second\n","[Step=2416]\tLoss=0.2312\tacc=0.9392\t2250.7 examples/second\n","[Step=2432]\tLoss=0.2306\tacc=0.9388\t2709.8 examples/second\n","[Step=2448]\tLoss=0.2290\tacc=0.9392\t2318.9 examples/second\n","[Step=2464]\tLoss=0.2330\tacc=0.9376\t2422.8 examples/second\n","[Step=2480]\tLoss=0.2339\tacc=0.9372\t2245.3 examples/second\n","[Step=2496]\tLoss=0.2311\tacc=0.9383\t2543.9 examples/second\n","[Step=2512]\tLoss=0.2304\tacc=0.9385\t1662.4 examples/second\n","[Step=2528]\tLoss=0.2324\tacc=0.9381\t1472.9 examples/second\n","[Step=2544]\tLoss=0.2314\tacc=0.9384\t1488.6 examples/second\n","[Step=2560]\tLoss=0.2318\tacc=0.9384\t1411.8 examples/second\n","[Step=2576]\tLoss=0.2310\tacc=0.9387\t1585.3 examples/second\n","[Step=2592]\tLoss=0.2299\tacc=0.9391\t2215.0 examples/second\n","[Step=2608]\tLoss=0.2316\tacc=0.9388\t2443.8 examples/second\n","[Step=2624]\tLoss=0.2301\tacc=0.9392\t2476.7 examples/second\n","[Step=2640]\tLoss=0.2306\tacc=0.9393\t2438.4 examples/second\n","[Step=2656]\tLoss=0.2297\tacc=0.9394\t2301.6 examples/second\n","[Step=2672]\tLoss=0.2298\tacc=0.9397\t2463.3 examples/second\n","[Step=2688]\tLoss=0.2302\tacc=0.9394\t2225.4 examples/second\n","[Step=2704]\tLoss=0.2292\tacc=0.9397\t2479.0 examples/second\n","[Step=2720]\tLoss=0.2303\tacc=0.9395\t4451.7 examples/second\n","[Step=2736]\tLoss=0.2301\tacc=0.9395\t5619.2 examples/second\n","Test Loss=0.3620, Test acc=0.8934\n","\n","Epoch: 7\n","[Step=2752]\tLoss=0.2201\tacc=0.9427\t270.5 examples/second\n","[Step=2768]\tLoss=0.2151\tacc=0.9448\t2410.4 examples/second\n","[Step=2784]\tLoss=0.2252\tacc=0.9417\t2267.8 examples/second\n","[Step=2800]\tLoss=0.2280\tacc=0.9396\t2406.5 examples/second\n","[Step=2816]\tLoss=0.2265\tacc=0.9403\t2397.3 examples/second\n","[Step=2832]\tLoss=0.2331\tacc=0.9380\t2298.9 examples/second\n","[Step=2848]\tLoss=0.2340\tacc=0.9376\t2670.6 examples/second\n","[Step=2864]\tLoss=0.2338\tacc=0.9376\t2292.8 examples/second\n","[Step=2880]\tLoss=0.2354\tacc=0.9371\t2488.8 examples/second\n","[Step=2896]\tLoss=0.2358\tacc=0.9368\t2296.2 examples/second\n","[Step=2912]\tLoss=0.2334\tacc=0.9377\t2343.3 examples/second\n","[Step=2928]\tLoss=0.2316\tacc=0.9381\t1806.8 examples/second\n","[Step=2944]\tLoss=0.2307\tacc=0.9387\t1442.2 examples/second\n","[Step=2960]\tLoss=0.2320\tacc=0.9382\t1522.8 examples/second\n","[Step=2976]\tLoss=0.2308\tacc=0.9383\t1475.6 examples/second\n","[Step=2992]\tLoss=0.2324\tacc=0.9378\t1406.7 examples/second\n","[Step=3008]\tLoss=0.2323\tacc=0.9378\t2117.3 examples/second\n","[Step=3024]\tLoss=0.2323\tacc=0.9380\t2534.8 examples/second\n","[Step=3040]\tLoss=0.2323\tacc=0.9380\t2523.3 examples/second\n","[Step=3056]\tLoss=0.2316\tacc=0.9385\t2497.0 examples/second\n","[Step=3072]\tLoss=0.2309\tacc=0.9388\t2174.3 examples/second\n","[Step=3088]\tLoss=0.2315\tacc=0.9387\t2146.5 examples/second\n","[Step=3104]\tLoss=0.2313\tacc=0.9387\t2602.9 examples/second\n","[Step=3120]\tLoss=0.2317\tacc=0.9386\t5723.3 examples/second\n","Test Loss=0.3601, Test acc=0.8965\n","Saving...\n","\n","Epoch: 8\n","[Step=3136]\tLoss=0.2371\tacc=0.9297\t291.4 examples/second\n","[Step=3152]\tLoss=0.2317\tacc=0.9336\t1256.9 examples/second\n","[Step=3168]\tLoss=0.2280\tacc=0.9391\t1643.6 examples/second\n","[Step=3184]\tLoss=0.2352\tacc=0.9364\t2403.0 examples/second\n","[Step=3200]\tLoss=0.2343\tacc=0.9367\t2322.4 examples/second\n","[Step=3216]\tLoss=0.2349\tacc=0.9371\t2669.8 examples/second\n","[Step=3232]\tLoss=0.2333\tacc=0.9374\t2437.4 examples/second\n","[Step=3248]\tLoss=0.2326\tacc=0.9378\t2301.8 examples/second\n","[Step=3264]\tLoss=0.2309\tacc=0.9387\t2607.4 examples/second\n","[Step=3280]\tLoss=0.2291\tacc=0.9390\t2361.3 examples/second\n","[Step=3296]\tLoss=0.2298\tacc=0.9387\t2357.0 examples/second\n","[Step=3312]\tLoss=0.2297\tacc=0.9389\t2338.2 examples/second\n","[Step=3328]\tLoss=0.2292\tacc=0.9393\t2406.9 examples/second\n","[Step=3344]\tLoss=0.2307\tacc=0.9387\t2533.4 examples/second\n","[Step=3360]\tLoss=0.2313\tacc=0.9381\t1719.8 examples/second\n","[Step=3376]\tLoss=0.2307\tacc=0.9384\t1371.9 examples/second\n","[Step=3392]\tLoss=0.2309\tacc=0.9387\t1409.2 examples/second\n","[Step=3408]\tLoss=0.2319\tacc=0.9381\t1254.7 examples/second\n","[Step=3424]\tLoss=0.2314\tacc=0.9383\t1355.8 examples/second\n","[Step=3440]\tLoss=0.2313\tacc=0.9384\t1334.4 examples/second\n","[Step=3456]\tLoss=0.2308\tacc=0.9383\t1272.8 examples/second\n","[Step=3472]\tLoss=0.2301\tacc=0.9386\t1447.6 examples/second\n","[Step=3488]\tLoss=0.2294\tacc=0.9388\t1429.0 examples/second\n","[Step=3504]\tLoss=0.2296\tacc=0.9386\t4298.4 examples/second\n","Test Loss=0.3616, Test acc=0.8961\n","\n","Epoch: 9\n","[Step=3520]\tLoss=0.2097\tacc=0.9531\t350.8 examples/second\n","[Step=3536]\tLoss=0.2470\tacc=0.9343\t1164.9 examples/second\n","[Step=3552]\tLoss=0.2431\tacc=0.9354\t1443.8 examples/second\n","[Step=3568]\tLoss=0.2375\tacc=0.9364\t1524.2 examples/second\n","[Step=3584]\tLoss=0.2354\tacc=0.9374\t1439.7 examples/second\n","[Step=3600]\tLoss=0.2281\tacc=0.9400\t1553.6 examples/second\n","[Step=3616]\tLoss=0.2279\tacc=0.9398\t2368.8 examples/second\n","[Step=3632]\tLoss=0.2286\tacc=0.9399\t2348.2 examples/second\n","[Step=3648]\tLoss=0.2282\tacc=0.9398\t2466.4 examples/second\n","[Step=3664]\tLoss=0.2261\tacc=0.9406\t2400.5 examples/second\n","[Step=3680]\tLoss=0.2271\tacc=0.9400\t2346.3 examples/second\n","[Step=3696]\tLoss=0.2287\tacc=0.9396\t2345.9 examples/second\n","[Step=3712]\tLoss=0.2284\tacc=0.9398\t2427.4 examples/second\n","[Step=3728]\tLoss=0.2269\tacc=0.9402\t2403.3 examples/second\n","[Step=3744]\tLoss=0.2278\tacc=0.9398\t2406.4 examples/second\n","[Step=3760]\tLoss=0.2287\tacc=0.9397\t2419.0 examples/second\n","[Step=3776]\tLoss=0.2296\tacc=0.9394\t2375.8 examples/second\n","[Step=3792]\tLoss=0.2305\tacc=0.9389\t1739.6 examples/second\n","[Step=3808]\tLoss=0.2295\tacc=0.9394\t1446.3 examples/second\n","[Step=3824]\tLoss=0.2294\tacc=0.9394\t1450.7 examples/second\n","[Step=3840]\tLoss=0.2292\tacc=0.9393\t1671.8 examples/second\n","[Step=3856]\tLoss=0.2286\tacc=0.9395\t1442.9 examples/second\n","[Step=3872]\tLoss=0.2286\tacc=0.9394\t2011.9 examples/second\n","[Step=3888]\tLoss=0.2292\tacc=0.9393\t3801.5 examples/second\n","[Step=3904]\tLoss=0.2292\tacc=0.9392\t5627.2 examples/second\n","Test Loss=0.3599, Test acc=0.8951\n","\n","Epoch: 10\n","[Step=3920]\tLoss=0.2341\tacc=0.9375\t352.5 examples/second\n","[Step=3936]\tLoss=0.2197\tacc=0.9408\t2250.0 examples/second\n","[Step=3952]\tLoss=0.2239\tacc=0.9399\t2602.4 examples/second\n","[Step=3968]\tLoss=0.2267\tacc=0.9399\t2180.1 examples/second\n","[Step=3984]\tLoss=0.2272\tacc=0.9397\t1468.2 examples/second\n","[Step=4000]\tLoss=0.2270\tacc=0.9386\t1361.1 examples/second\n","[Step=4016]\tLoss=0.2259\tacc=0.9386\t1703.8 examples/second\n","[Step=4032]\tLoss=0.2275\tacc=0.9385\t1428.5 examples/second\n","[Step=4048]\tLoss=0.2272\tacc=0.9385\t2189.1 examples/second\n","[Step=4064]\tLoss=0.2270\tacc=0.9389\t2241.4 examples/second\n","[Step=4080]\tLoss=0.2268\tacc=0.9390\t2464.7 examples/second\n","[Step=4096]\tLoss=0.2281\tacc=0.9384\t2479.7 examples/second\n","[Step=4112]\tLoss=0.2311\tacc=0.9372\t2384.9 examples/second\n","[Step=4128]\tLoss=0.2313\tacc=0.9372\t2299.0 examples/second\n","[Step=4144]\tLoss=0.2308\tacc=0.9375\t2395.9 examples/second\n","[Step=4160]\tLoss=0.2319\tacc=0.9369\t2510.1 examples/second\n","[Step=4176]\tLoss=0.2309\tacc=0.9374\t2451.7 examples/second\n","[Step=4192]\tLoss=0.2302\tacc=0.9377\t2358.1 examples/second\n","[Step=4208]\tLoss=0.2316\tacc=0.9374\t2176.7 examples/second\n","[Step=4224]\tLoss=0.2318\tacc=0.9374\t1543.1 examples/second\n","[Step=4240]\tLoss=0.2312\tacc=0.9375\t1519.9 examples/second\n","[Step=4256]\tLoss=0.2315\tacc=0.9375\t1459.6 examples/second\n","[Step=4272]\tLoss=0.2310\tacc=0.9379\t1670.4 examples/second\n","[Step=4288]\tLoss=0.2307\tacc=0.9381\t4900.8 examples/second\n","Test Loss=0.3620, Test acc=0.8953\n","\n","Epoch: 11\n","[Step=4304]\tLoss=0.1838\tacc=0.9531\t379.6 examples/second\n","[Step=4320]\tLoss=0.2267\tacc=0.9391\t1490.7 examples/second\n","[Step=4336]\tLoss=0.2332\tacc=0.9368\t2365.8 examples/second\n","[Step=4352]\tLoss=0.2349\tacc=0.9369\t2369.5 examples/second\n","[Step=4368]\tLoss=0.2313\tacc=0.9384\t2322.3 examples/second\n","[Step=4384]\tLoss=0.2329\tacc=0.9382\t2238.5 examples/second\n","[Step=4400]\tLoss=0.2317\tacc=0.9390\t2269.6 examples/second\n","[Step=4416]\tLoss=0.2310\tacc=0.9389\t1497.5 examples/second\n","[Step=4432]\tLoss=0.2297\tacc=0.9391\t1443.2 examples/second\n","[Step=4448]\tLoss=0.2292\tacc=0.9394\t1594.7 examples/second\n","[Step=4464]\tLoss=0.2288\tacc=0.9394\t1481.2 examples/second\n","[Step=4480]\tLoss=0.2287\tacc=0.9392\t2168.3 examples/second\n","[Step=4496]\tLoss=0.2275\tacc=0.9396\t2396.5 examples/second\n","[Step=4512]\tLoss=0.2274\tacc=0.9399\t2322.6 examples/second\n","[Step=4528]\tLoss=0.2270\tacc=0.9396\t2468.0 examples/second\n","[Step=4544]\tLoss=0.2267\tacc=0.9397\t2361.4 examples/second\n","[Step=4560]\tLoss=0.2277\tacc=0.9392\t2515.3 examples/second\n","[Step=4576]\tLoss=0.2277\tacc=0.9391\t2189.6 examples/second\n","[Step=4592]\tLoss=0.2280\tacc=0.9390\t2459.8 examples/second\n","[Step=4608]\tLoss=0.2279\tacc=0.9391\t2368.1 examples/second\n","[Step=4624]\tLoss=0.2278\tacc=0.9393\t1944.8 examples/second\n","[Step=4640]\tLoss=0.2283\tacc=0.9395\t2131.8 examples/second\n","[Step=4656]\tLoss=0.2283\tacc=0.9397\t2022.4 examples/second\n","[Step=4672]\tLoss=0.2286\tacc=0.9394\t2608.7 examples/second\n","[Step=4688]\tLoss=0.2292\tacc=0.9390\t5506.4 examples/second\n","Test Loss=0.3603, Test acc=0.8947\n","\n","Epoch: 12\n","[Step=4704]\tLoss=0.2252\tacc=0.9382\t298.2 examples/second\n","[Step=4720]\tLoss=0.2307\tacc=0.9372\t2507.8 examples/second\n","[Step=4736]\tLoss=0.2272\tacc=0.9393\t2277.9 examples/second\n","[Step=4752]\tLoss=0.2270\tacc=0.9391\t2488.5 examples/second\n","[Step=4768]\tLoss=0.2306\tacc=0.9385\t2268.1 examples/second\n","[Step=4784]\tLoss=0.2314\tacc=0.9381\t2327.6 examples/second\n","[Step=4800]\tLoss=0.2322\tacc=0.9384\t2080.1 examples/second\n","[Step=4816]\tLoss=0.2358\tacc=0.9376\t1626.2 examples/second\n","[Step=4832]\tLoss=0.2357\tacc=0.9377\t1354.2 examples/second\n","[Step=4848]\tLoss=0.2341\tacc=0.9381\t1527.0 examples/second\n","[Step=4864]\tLoss=0.2334\tacc=0.9384\t1478.4 examples/second\n","[Step=4880]\tLoss=0.2334\tacc=0.9385\t1653.4 examples/second\n","[Step=4896]\tLoss=0.2335\tacc=0.9381\t2372.8 examples/second\n","[Step=4912]\tLoss=0.2306\tacc=0.9391\t2368.3 examples/second\n","[Step=4928]\tLoss=0.2308\tacc=0.9388\t2413.8 examples/second\n","[Step=4944]\tLoss=0.2316\tacc=0.9382\t2417.3 examples/second\n","[Step=4960]\tLoss=0.2316\tacc=0.9379\t2375.0 examples/second\n","[Step=4976]\tLoss=0.2311\tacc=0.9379\t2306.4 examples/second\n","[Step=4992]\tLoss=0.2327\tacc=0.9378\t2407.4 examples/second\n","[Step=5008]\tLoss=0.2333\tacc=0.9376\t2387.1 examples/second\n","[Step=5024]\tLoss=0.2331\tacc=0.9376\t2309.0 examples/second\n","[Step=5040]\tLoss=0.2329\tacc=0.9377\t2060.8 examples/second\n","[Step=5056]\tLoss=0.2327\tacc=0.9378\t2269.0 examples/second\n","[Step=5072]\tLoss=0.2326\tacc=0.9375\t5501.6 examples/second\n","Test Loss=0.3637, Test acc=0.8954\n","\n","Epoch: 13\n","[Step=5088]\tLoss=0.2213\tacc=0.9375\t301.6 examples/second\n","[Step=5104]\tLoss=0.2312\tacc=0.9401\t1621.5 examples/second\n","[Step=5120]\tLoss=0.2273\tacc=0.9411\t2321.3 examples/second\n","[Step=5136]\tLoss=0.2280\tacc=0.9402\t2428.5 examples/second\n","[Step=5152]\tLoss=0.2308\tacc=0.9392\t2405.2 examples/second\n","[Step=5168]\tLoss=0.2279\tacc=0.9403\t2200.3 examples/second\n","[Step=5184]\tLoss=0.2299\tacc=0.9397\t2390.6 examples/second\n","[Step=5200]\tLoss=0.2293\tacc=0.9400\t2351.7 examples/second\n","[Step=5216]\tLoss=0.2284\tacc=0.9403\t2034.1 examples/second\n","[Step=5232]\tLoss=0.2287\tacc=0.9404\t1536.3 examples/second\n","[Step=5248]\tLoss=0.2274\tacc=0.9411\t1422.7 examples/second\n","[Step=5264]\tLoss=0.2286\tacc=0.9408\t1615.4 examples/second\n","[Step=5280]\tLoss=0.2298\tacc=0.9402\t1573.0 examples/second\n","[Step=5296]\tLoss=0.2300\tacc=0.9400\t1972.4 examples/second\n","[Step=5312]\tLoss=0.2301\tacc=0.9400\t2549.4 examples/second\n","[Step=5328]\tLoss=0.2295\tacc=0.9400\t2326.2 examples/second\n","[Step=5344]\tLoss=0.2284\tacc=0.9402\t2265.6 examples/second\n","[Step=5360]\tLoss=0.2289\tacc=0.9400\t2559.3 examples/second\n","[Step=5376]\tLoss=0.2297\tacc=0.9398\t2366.2 examples/second\n","[Step=5392]\tLoss=0.2299\tacc=0.9393\t2285.7 examples/second\n","[Step=5408]\tLoss=0.2304\tacc=0.9390\t2438.2 examples/second\n","[Step=5424]\tLoss=0.2312\tacc=0.9386\t2527.2 examples/second\n","[Step=5440]\tLoss=0.2321\tacc=0.9383\t2351.0 examples/second\n","[Step=5456]\tLoss=0.2331\tacc=0.9378\t4423.2 examples/second\n","[Step=5472]\tLoss=0.2322\tacc=0.9384\t5663.6 examples/second\n","Test Loss=0.3605, Test acc=0.8955\n","\n","Epoch: 14\n","[Step=5488]\tLoss=0.2370\tacc=0.9353\t262.9 examples/second\n","[Step=5504]\tLoss=0.2511\tacc=0.9294\t2385.0 examples/second\n","[Step=5520]\tLoss=0.2389\tacc=0.9333\t2301.4 examples/second\n","[Step=5536]\tLoss=0.2337\tacc=0.9352\t2479.9 examples/second\n","[Step=5552]\tLoss=0.2315\tacc=0.9368\t2317.2 examples/second\n","[Step=5568]\tLoss=0.2310\tacc=0.9380\t2560.7 examples/second\n","[Step=5584]\tLoss=0.2311\tacc=0.9381\t2347.8 examples/second\n","[Step=5600]\tLoss=0.2346\tacc=0.9369\t2382.7 examples/second\n","[Step=5616]\tLoss=0.2356\tacc=0.9365\t2537.7 examples/second\n","[Step=5632]\tLoss=0.2351\tacc=0.9372\t2196.5 examples/second\n","[Step=5648]\tLoss=0.2347\tacc=0.9370\t1372.3 examples/second\n","[Step=5664]\tLoss=0.2351\tacc=0.9368\t1464.3 examples/second\n","[Step=5680]\tLoss=0.2359\tacc=0.9364\t1522.3 examples/second\n","[Step=5696]\tLoss=0.2355\tacc=0.9366\t1532.3 examples/second\n","[Step=5712]\tLoss=0.2346\tacc=0.9368\t1630.7 examples/second\n","[Step=5728]\tLoss=0.2350\tacc=0.9369\t1557.9 examples/second\n","[Step=5744]\tLoss=0.2345\tacc=0.9370\t1538.1 examples/second\n","[Step=5760]\tLoss=0.2345\tacc=0.9371\t1571.4 examples/second\n","[Step=5776]\tLoss=0.2346\tacc=0.9372\t1569.4 examples/second\n","[Step=5792]\tLoss=0.2350\tacc=0.9368\t2444.5 examples/second\n","[Step=5808]\tLoss=0.2349\tacc=0.9369\t2298.3 examples/second\n","[Step=5824]\tLoss=0.2345\tacc=0.9371\t2357.3 examples/second\n","[Step=5840]\tLoss=0.2336\tacc=0.9376\t2224.0 examples/second\n","[Step=5856]\tLoss=0.2346\tacc=0.9372\t5144.6 examples/second\n","Test Loss=0.3599, Test acc=0.8954\n","\n","Epoch: 15\n","[Step=5872]\tLoss=0.2315\tacc=0.9319\t236.0 examples/second\n","[Step=5888]\tLoss=0.2509\tacc=0.9331\t1840.9 examples/second\n","[Step=5904]\tLoss=0.2397\tacc=0.9363\t2467.5 examples/second\n","[Step=5920]\tLoss=0.2393\tacc=0.9358\t2467.0 examples/second\n","[Step=5936]\tLoss=0.2301\tacc=0.9393\t2484.5 examples/second\n","[Step=5952]\tLoss=0.2297\tacc=0.9387\t2264.7 examples/second\n","[Step=5968]\tLoss=0.2284\tacc=0.9391\t2314.1 examples/second\n","[Step=5984]\tLoss=0.2259\tacc=0.9403\t2596.5 examples/second\n","[Step=6000]\tLoss=0.2269\tacc=0.9402\t2236.1 examples/second\n","[Step=6016]\tLoss=0.2257\tacc=0.9410\t2418.4 examples/second\n","[Step=6032]\tLoss=0.2254\tacc=0.9410\t2042.4 examples/second\n","[Step=6048]\tLoss=0.2256\tacc=0.9407\t1397.9 examples/second\n","[Step=6064]\tLoss=0.2270\tacc=0.9405\t1523.1 examples/second\n","[Step=6080]\tLoss=0.2266\tacc=0.9406\t1571.1 examples/second\n","[Step=6096]\tLoss=0.2285\tacc=0.9399\t1478.4 examples/second\n","[Step=6112]\tLoss=0.2304\tacc=0.9394\t2238.6 examples/second\n","[Step=6128]\tLoss=0.2301\tacc=0.9390\t2424.8 examples/second\n","[Step=6144]\tLoss=0.2308\tacc=0.9387\t2486.7 examples/second\n","[Step=6160]\tLoss=0.2302\tacc=0.9390\t2396.3 examples/second\n","[Step=6176]\tLoss=0.2298\tacc=0.9390\t2229.2 examples/second\n","[Step=6192]\tLoss=0.2298\tacc=0.9393\t2440.2 examples/second\n","[Step=6208]\tLoss=0.2306\tacc=0.9391\t2309.9 examples/second\n","[Step=6224]\tLoss=0.2308\tacc=0.9392\t2398.4 examples/second\n","[Step=6240]\tLoss=0.2305\tacc=0.9393\t4908.5 examples/second\n","[Step=6256]\tLoss=0.2309\tacc=0.9392\t5657.1 examples/second\n","Test Loss=0.3607, Test acc=0.8959\n","\n","Epoch: 16\n","[Step=6272]\tLoss=0.2321\tacc=0.9326\t262.3 examples/second\n","[Step=6288]\tLoss=0.2266\tacc=0.9363\t2422.7 examples/second\n","[Step=6304]\tLoss=0.2297\tacc=0.9364\t2410.1 examples/second\n","[Step=6320]\tLoss=0.2283\tacc=0.9370\t2407.0 examples/second\n","[Step=6336]\tLoss=0.2272\tacc=0.9377\t2387.7 examples/second\n","[Step=6352]\tLoss=0.2270\tacc=0.9385\t2415.4 examples/second\n","[Step=6368]\tLoss=0.2271\tacc=0.9387\t2387.9 examples/second\n","[Step=6384]\tLoss=0.2278\tacc=0.9385\t2372.6 examples/second\n","[Step=6400]\tLoss=0.2267\tacc=0.9385\t2490.4 examples/second\n","[Step=6416]\tLoss=0.2273\tacc=0.9387\t2522.7 examples/second\n","[Step=6432]\tLoss=0.2282\tacc=0.9382\t2291.4 examples/second\n","[Step=6448]\tLoss=0.2299\tacc=0.9375\t1686.0 examples/second\n","[Step=6464]\tLoss=0.2313\tacc=0.9369\t1501.6 examples/second\n","[Step=6480]\tLoss=0.2321\tacc=0.9364\t1525.5 examples/second\n","[Step=6496]\tLoss=0.2320\tacc=0.9365\t1484.7 examples/second\n","[Step=6512]\tLoss=0.2319\tacc=0.9369\t1727.1 examples/second\n","[Step=6528]\tLoss=0.2310\tacc=0.9376\t2557.9 examples/second\n","[Step=6544]\tLoss=0.2297\tacc=0.9383\t2278.3 examples/second\n","[Step=6560]\tLoss=0.2293\tacc=0.9385\t2435.8 examples/second\n","[Step=6576]\tLoss=0.2283\tacc=0.9387\t2499.4 examples/second\n","[Step=6592]\tLoss=0.2277\tacc=0.9389\t1808.0 examples/second\n","[Step=6608]\tLoss=0.2281\tacc=0.9388\t2393.0 examples/second\n","[Step=6624]\tLoss=0.2279\tacc=0.9390\t3221.6 examples/second\n","[Step=6640]\tLoss=0.2278\tacc=0.9390\t5693.1 examples/second\n","Test Loss=0.3614, Test acc=0.8962\n","\n","Epoch: 17\n","[Step=6656]\tLoss=0.2324\tacc=0.9349\t282.5 examples/second\n","[Step=6672]\tLoss=0.2243\tacc=0.9378\t1780.1 examples/second\n","[Step=6688]\tLoss=0.2253\tacc=0.9381\t2306.7 examples/second\n","[Step=6704]\tLoss=0.2256\tacc=0.9386\t2374.4 examples/second\n","[Step=6720]\tLoss=0.2289\tacc=0.9372\t2425.4 examples/second\n","[Step=6736]\tLoss=0.2313\tacc=0.9371\t2319.6 examples/second\n","[Step=6752]\tLoss=0.2317\tacc=0.9368\t2452.9 examples/second\n","[Step=6768]\tLoss=0.2325\tacc=0.9367\t2404.2 examples/second\n","[Step=6784]\tLoss=0.2331\tacc=0.9368\t2470.8 examples/second\n","[Step=6800]\tLoss=0.2310\tacc=0.9370\t2278.4 examples/second\n","[Step=6816]\tLoss=0.2301\tacc=0.9375\t2492.0 examples/second\n","[Step=6832]\tLoss=0.2315\tacc=0.9373\t2283.3 examples/second\n","[Step=6848]\tLoss=0.2315\tacc=0.9375\t2161.1 examples/second\n","[Step=6864]\tLoss=0.2321\tacc=0.9376\t1413.6 examples/second\n","[Step=6880]\tLoss=0.2313\tacc=0.9379\t1425.6 examples/second\n","[Step=6896]\tLoss=0.2314\tacc=0.9376\t1637.6 examples/second\n","[Step=6912]\tLoss=0.2320\tacc=0.9374\t1480.6 examples/second\n","[Step=6928]\tLoss=0.2312\tacc=0.9375\t2184.6 examples/second\n","[Step=6944]\tLoss=0.2315\tacc=0.9375\t2359.9 examples/second\n","[Step=6960]\tLoss=0.2306\tacc=0.9375\t2461.8 examples/second\n","[Step=6976]\tLoss=0.2315\tacc=0.9372\t2430.7 examples/second\n","[Step=6992]\tLoss=0.2304\tacc=0.9375\t2339.0 examples/second\n","[Step=7008]\tLoss=0.2301\tacc=0.9375\t2420.3 examples/second\n","[Step=7024]\tLoss=0.2294\tacc=0.9377\t5767.9 examples/second\n","Test Loss=0.3593, Test acc=0.8953\n","\n","Epoch: 18\n","[Step=7040]\tLoss=0.1326\tacc=0.9727\t373.6 examples/second\n","[Step=7056]\tLoss=0.2134\tacc=0.9418\t852.8 examples/second\n","[Step=7072]\tLoss=0.2257\tacc=0.9416\t1529.5 examples/second\n","[Step=7088]\tLoss=0.2332\tacc=0.9383\t1625.8 examples/second\n","[Step=7104]\tLoss=0.2313\tacc=0.9380\t2124.5 examples/second\n","[Step=7120]\tLoss=0.2326\tacc=0.9381\t2424.5 examples/second\n","[Step=7136]\tLoss=0.2328\tacc=0.9382\t2482.6 examples/second\n","[Step=7152]\tLoss=0.2343\tacc=0.9371\t2341.6 examples/second\n","[Step=7168]\tLoss=0.2341\tacc=0.9369\t2435.0 examples/second\n","[Step=7184]\tLoss=0.2336\tacc=0.9369\t2297.9 examples/second\n","[Step=7200]\tLoss=0.2350\tacc=0.9362\t2555.5 examples/second\n","[Step=7216]\tLoss=0.2320\tacc=0.9371\t2400.7 examples/second\n","[Step=7232]\tLoss=0.2337\tacc=0.9368\t2346.4 examples/second\n","[Step=7248]\tLoss=0.2335\tacc=0.9370\t2465.7 examples/second\n","[Step=7264]\tLoss=0.2335\tacc=0.9371\t2311.0 examples/second\n","[Step=7280]\tLoss=0.2339\tacc=0.9369\t2039.5 examples/second\n","[Step=7296]\tLoss=0.2338\tacc=0.9366\t1525.0 examples/second\n","[Step=7312]\tLoss=0.2336\tacc=0.9369\t1525.3 examples/second\n","[Step=7328]\tLoss=0.2338\tacc=0.9369\t1554.1 examples/second\n","[Step=7344]\tLoss=0.2338\tacc=0.9369\t1463.1 examples/second\n","[Step=7360]\tLoss=0.2337\tacc=0.9371\t2055.1 examples/second\n","[Step=7376]\tLoss=0.2326\tacc=0.9377\t2449.1 examples/second\n","[Step=7392]\tLoss=0.2323\tacc=0.9380\t2291.2 examples/second\n","[Step=7408]\tLoss=0.2324\tacc=0.9381\t4010.2 examples/second\n","[Step=7424]\tLoss=0.2329\tacc=0.9380\t5617.2 examples/second\n","Test Loss=0.3609, Test acc=0.8950\n","\n","Epoch: 19\n","[Step=7440]\tLoss=0.2600\tacc=0.9268\t343.9 examples/second\n","[Step=7456]\tLoss=0.2378\tacc=0.9340\t1933.1 examples/second\n","[Step=7472]\tLoss=0.2294\tacc=0.9364\t1416.2 examples/second\n","[Step=7488]\tLoss=0.2339\tacc=0.9343\t1479.1 examples/second\n","[Step=7504]\tLoss=0.2350\tacc=0.9351\t1443.0 examples/second\n","[Step=7520]\tLoss=0.2358\tacc=0.9356\t1424.3 examples/second\n","[Step=7536]\tLoss=0.2373\tacc=0.9352\t2352.0 examples/second\n","[Step=7552]\tLoss=0.2336\tacc=0.9365\t2444.0 examples/second\n","[Step=7568]\tLoss=0.2362\tacc=0.9356\t2461.9 examples/second\n","[Step=7584]\tLoss=0.2357\tacc=0.9360\t2271.3 examples/second\n","[Step=7600]\tLoss=0.2341\tacc=0.9368\t2418.3 examples/second\n","[Step=7616]\tLoss=0.2331\tacc=0.9366\t2294.7 examples/second\n","[Step=7632]\tLoss=0.2310\tacc=0.9376\t2664.2 examples/second\n","[Step=7648]\tLoss=0.2320\tacc=0.9371\t2204.2 examples/second\n","[Step=7664]\tLoss=0.2334\tacc=0.9367\t2448.5 examples/second\n","[Step=7680]\tLoss=0.2323\tacc=0.9373\t2338.2 examples/second\n","[Step=7696]\tLoss=0.2329\tacc=0.9369\t2466.4 examples/second\n","[Step=7712]\tLoss=0.2314\tacc=0.9375\t1714.9 examples/second\n","[Step=7728]\tLoss=0.2321\tacc=0.9372\t1443.8 examples/second\n","[Step=7744]\tLoss=0.2315\tacc=0.9375\t1548.4 examples/second\n","[Step=7760]\tLoss=0.2314\tacc=0.9375\t1618.7 examples/second\n","[Step=7776]\tLoss=0.2311\tacc=0.9377\t1513.2 examples/second\n","[Step=7792]\tLoss=0.2307\tacc=0.9378\t2698.8 examples/second\n","[Step=7808]\tLoss=0.2315\tacc=0.9373\t5643.8 examples/second\n","Test Loss=0.3645, Test acc=0.8946\n","\n","Epoch: 20\n","[Step=7824]\tLoss=0.2151\tacc=0.9453\t375.7 examples/second\n","[Step=7840]\tLoss=0.2537\tacc=0.9309\t1466.0 examples/second\n","[Step=7856]\tLoss=0.2461\tacc=0.9342\t2588.9 examples/second\n","[Step=7872]\tLoss=0.2393\tacc=0.9367\t2159.4 examples/second\n","[Step=7888]\tLoss=0.2394\tacc=0.9365\t1594.9 examples/second\n","[Step=7904]\tLoss=0.2371\tacc=0.9371\t1330.8 examples/second\n","[Step=7920]\tLoss=0.2338\tacc=0.9380\t1450.0 examples/second\n","[Step=7936]\tLoss=0.2352\tacc=0.9372\t1567.2 examples/second\n","[Step=7952]\tLoss=0.2335\tacc=0.9376\t1942.5 examples/second\n","[Step=7968]\tLoss=0.2323\tacc=0.9385\t2008.0 examples/second\n","[Step=7984]\tLoss=0.2322\tacc=0.9384\t1538.8 examples/second\n","[Step=8000]\tLoss=0.2313\tacc=0.9390\t1528.8 examples/second\n","[Step=8016]\tLoss=0.2319\tacc=0.9389\t1532.8 examples/second\n","[Step=8032]\tLoss=0.2304\tacc=0.9393\t1472.0 examples/second\n","[Step=8048]\tLoss=0.2308\tacc=0.9391\t1867.9 examples/second\n","[Step=8064]\tLoss=0.2308\tacc=0.9391\t2585.5 examples/second\n","[Step=8080]\tLoss=0.2312\tacc=0.9387\t2200.8 examples/second\n","[Step=8096]\tLoss=0.2299\tacc=0.9391\t1738.2 examples/second\n","[Step=8112]\tLoss=0.2301\tacc=0.9391\t1438.1 examples/second\n","[Step=8128]\tLoss=0.2304\tacc=0.9390\t1484.6 examples/second\n","[Step=8144]\tLoss=0.2304\tacc=0.9390\t1329.5 examples/second\n","[Step=8160]\tLoss=0.2298\tacc=0.9393\t1329.5 examples/second\n","[Step=8176]\tLoss=0.2310\tacc=0.9389\t1356.7 examples/second\n","[Step=8192]\tLoss=0.2326\tacc=0.9384\t2880.3 examples/second\n","[Step=8208]\tLoss=0.2336\tacc=0.9380\t5405.6 examples/second\n","Test Loss=0.3629, Test acc=0.8956\n","\n","Epoch: 21\n","[Step=8224]\tLoss=0.2270\tacc=0.9387\t304.8 examples/second\n","[Step=8240]\tLoss=0.2387\tacc=0.9388\t2416.1 examples/second\n","[Step=8256]\tLoss=0.2409\tacc=0.9359\t2198.4 examples/second\n","[Step=8272]\tLoss=0.2448\tacc=0.9342\t1316.7 examples/second\n","[Step=8288]\tLoss=0.2380\tacc=0.9367\t1458.3 examples/second\n","[Step=8304]\tLoss=0.2358\tacc=0.9379\t1341.2 examples/second\n","[Step=8320]\tLoss=0.2347\tacc=0.9376\t1407.2 examples/second\n","[Step=8336]\tLoss=0.2350\tacc=0.9372\t1989.1 examples/second\n","[Step=8352]\tLoss=0.2358\tacc=0.9371\t2412.4 examples/second\n","[Step=8368]\tLoss=0.2372\tacc=0.9362\t2309.8 examples/second\n","[Step=8384]\tLoss=0.2364\tacc=0.9359\t2483.8 examples/second\n","[Step=8400]\tLoss=0.2352\tacc=0.9367\t2285.0 examples/second\n","[Step=8416]\tLoss=0.2351\tacc=0.9365\t2450.2 examples/second\n","[Step=8432]\tLoss=0.2352\tacc=0.9364\t2377.1 examples/second\n","[Step=8448]\tLoss=0.2356\tacc=0.9360\t2432.3 examples/second\n","[Step=8464]\tLoss=0.2351\tacc=0.9363\t2356.9 examples/second\n","[Step=8480]\tLoss=0.2342\tacc=0.9368\t2382.2 examples/second\n","[Step=8496]\tLoss=0.2331\tacc=0.9372\t2281.7 examples/second\n","[Step=8512]\tLoss=0.2323\tacc=0.9374\t2392.0 examples/second\n","[Step=8528]\tLoss=0.2325\tacc=0.9373\t1554.0 examples/second\n","[Step=8544]\tLoss=0.2322\tacc=0.9374\t1422.8 examples/second\n","[Step=8560]\tLoss=0.2321\tacc=0.9375\t1570.3 examples/second\n","[Step=8576]\tLoss=0.2314\tacc=0.9377\t1799.1 examples/second\n","[Step=8592]\tLoss=0.2307\tacc=0.9379\t5125.4 examples/second\n","Test Loss=0.3648, Test acc=0.8927\n","\n","Epoch: 22\n","[Step=8608]\tLoss=0.2098\tacc=0.9518\t355.9 examples/second\n","[Step=8624]\tLoss=0.2312\tacc=0.9421\t1893.4 examples/second\n","[Step=8640]\tLoss=0.2253\tacc=0.9416\t2322.1 examples/second\n","[Step=8656]\tLoss=0.2282\tacc=0.9405\t2326.0 examples/second\n","[Step=8672]\tLoss=0.2231\tacc=0.9424\t2450.3 examples/second\n","[Step=8688]\tLoss=0.2239\tacc=0.9417\t2385.3 examples/second\n","[Step=8704]\tLoss=0.2250\tacc=0.9413\t1840.2 examples/second\n","[Step=8720]\tLoss=0.2252\tacc=0.9414\t1473.1 examples/second\n","[Step=8736]\tLoss=0.2281\tacc=0.9402\t1535.2 examples/second\n","[Step=8752]\tLoss=0.2275\tacc=0.9406\t1535.2 examples/second\n","[Step=8768]\tLoss=0.2267\tacc=0.9409\t1600.8 examples/second\n","[Step=8784]\tLoss=0.2265\tacc=0.9409\t2346.4 examples/second\n","[Step=8800]\tLoss=0.2269\tacc=0.9408\t2278.6 examples/second\n","[Step=8816]\tLoss=0.2270\tacc=0.9405\t2562.5 examples/second\n","[Step=8832]\tLoss=0.2275\tacc=0.9405\t2238.4 examples/second\n","[Step=8848]\tLoss=0.2281\tacc=0.9401\t2460.4 examples/second\n","[Step=8864]\tLoss=0.2284\tacc=0.9401\t2323.2 examples/second\n","[Step=8880]\tLoss=0.2274\tacc=0.9405\t2517.9 examples/second\n","[Step=8896]\tLoss=0.2272\tacc=0.9406\t2498.5 examples/second\n","[Step=8912]\tLoss=0.2279\tacc=0.9404\t2300.1 examples/second\n","[Step=8928]\tLoss=0.2274\tacc=0.9404\t2296.5 examples/second\n","[Step=8944]\tLoss=0.2289\tacc=0.9400\t2259.2 examples/second\n","[Step=8960]\tLoss=0.2284\tacc=0.9401\t1934.2 examples/second\n","[Step=8976]\tLoss=0.2274\tacc=0.9405\t3296.7 examples/second\n","[Step=8992]\tLoss=0.2280\tacc=0.9400\t5527.2 examples/second\n","Test Loss=0.3608, Test acc=0.8946\n","\n","Epoch: 23\n","[Step=9008]\tLoss=0.2352\tacc=0.9307\t284.1 examples/second\n","[Step=9024]\tLoss=0.2290\tacc=0.9360\t2354.1 examples/second\n","[Step=9040]\tLoss=0.2243\tacc=0.9378\t2367.1 examples/second\n","[Step=9056]\tLoss=0.2238\tacc=0.9390\t2465.4 examples/second\n","[Step=9072]\tLoss=0.2239\tacc=0.9400\t2356.9 examples/second\n","[Step=9088]\tLoss=0.2209\tacc=0.9408\t2472.5 examples/second\n","[Step=9104]\tLoss=0.2234\tacc=0.9405\t2277.7 examples/second\n","[Step=9120]\tLoss=0.2236\tacc=0.9405\t2030.7 examples/second\n","[Step=9136]\tLoss=0.2244\tacc=0.9402\t1372.2 examples/second\n","[Step=9152]\tLoss=0.2252\tacc=0.9402\t1446.8 examples/second\n","[Step=9168]\tLoss=0.2261\tacc=0.9392\t1587.3 examples/second\n","[Step=9184]\tLoss=0.2279\tacc=0.9388\t1679.2 examples/second\n","[Step=9200]\tLoss=0.2269\tacc=0.9395\t2344.6 examples/second\n","[Step=9216]\tLoss=0.2272\tacc=0.9398\t2425.9 examples/second\n","[Step=9232]\tLoss=0.2282\tacc=0.9392\t2415.4 examples/second\n","[Step=9248]\tLoss=0.2278\tacc=0.9395\t2415.4 examples/second\n","[Step=9264]\tLoss=0.2270\tacc=0.9398\t2367.9 examples/second\n","[Step=9280]\tLoss=0.2274\tacc=0.9399\t2319.0 examples/second\n","[Step=9296]\tLoss=0.2267\tacc=0.9401\t2292.3 examples/second\n","[Step=9312]\tLoss=0.2271\tacc=0.9399\t2478.6 examples/second\n","[Step=9328]\tLoss=0.2279\tacc=0.9396\t2414.4 examples/second\n","[Step=9344]\tLoss=0.2274\tacc=0.9399\t2528.4 examples/second\n","[Step=9360]\tLoss=0.2280\tacc=0.9398\t3069.9 examples/second\n","[Step=9376]\tLoss=0.2284\tacc=0.9396\t5671.7 examples/second\n","Test Loss=0.3634, Test acc=0.8957\n","\n","Epoch: 24\n","[Step=9392]\tLoss=0.2465\tacc=0.9355\t279.9 examples/second\n","[Step=9408]\tLoss=0.2329\tacc=0.9391\t1777.9 examples/second\n","[Step=9424]\tLoss=0.2318\tacc=0.9416\t2667.9 examples/second\n","[Step=9440]\tLoss=0.2315\tacc=0.9407\t2307.2 examples/second\n","[Step=9456]\tLoss=0.2314\tacc=0.9409\t2366.2 examples/second\n","[Step=9472]\tLoss=0.2314\tacc=0.9404\t2170.6 examples/second\n","[Step=9488]\tLoss=0.2268\tacc=0.9414\t2395.4 examples/second\n","[Step=9504]\tLoss=0.2259\tacc=0.9417\t2421.9 examples/second\n","[Step=9520]\tLoss=0.2261\tacc=0.9421\t2278.3 examples/second\n","[Step=9536]\tLoss=0.2252\tacc=0.9423\t1475.8 examples/second\n","[Step=9552]\tLoss=0.2289\tacc=0.9408\t1470.0 examples/second\n","[Step=9568]\tLoss=0.2287\tacc=0.9406\t1510.3 examples/second\n","[Step=9584]\tLoss=0.2276\tacc=0.9411\t1399.3 examples/second\n","[Step=9600]\tLoss=0.2284\tacc=0.9406\t2102.6 examples/second\n","[Step=9616]\tLoss=0.2284\tacc=0.9404\t2677.6 examples/second\n","[Step=9632]\tLoss=0.2285\tacc=0.9402\t2342.4 examples/second\n","[Step=9648]\tLoss=0.2295\tacc=0.9395\t2360.0 examples/second\n","[Step=9664]\tLoss=0.2307\tacc=0.9392\t2452.9 examples/second\n","[Step=9680]\tLoss=0.2291\tacc=0.9397\t2426.9 examples/second\n","[Step=9696]\tLoss=0.2304\tacc=0.9391\t2329.2 examples/second\n","[Step=9712]\tLoss=0.2301\tacc=0.9392\t2232.1 examples/second\n","[Step=9728]\tLoss=0.2309\tacc=0.9388\t2432.5 examples/second\n","[Step=9744]\tLoss=0.2303\tacc=0.9391\t2412.6 examples/second\n","[Step=9760]\tLoss=0.2299\tacc=0.9393\t5439.4 examples/second\n","Test Loss=0.3622, Test acc=0.8941\n","\n","Epoch: 25\n","[Step=9776]\tLoss=0.2212\tacc=0.9219\t297.6 examples/second\n","[Step=9792]\tLoss=0.2188\tacc=0.9435\t1424.9 examples/second\n","[Step=9808]\tLoss=0.2293\tacc=0.9396\t2409.8 examples/second\n","[Step=9824]\tLoss=0.2324\tacc=0.9377\t2409.8 examples/second\n","[Step=9840]\tLoss=0.2320\tacc=0.9385\t2422.6 examples/second\n","[Step=9856]\tLoss=0.2338\tacc=0.9388\t2413.0 examples/second\n","[Step=9872]\tLoss=0.2307\tacc=0.9393\t2413.2 examples/second\n","[Step=9888]\tLoss=0.2320\tacc=0.9384\t2323.1 examples/second\n","[Step=9904]\tLoss=0.2303\tacc=0.9394\t2387.4 examples/second\n","[Step=9920]\tLoss=0.2288\tacc=0.9397\t2464.9 examples/second\n","[Step=9936]\tLoss=0.2286\tacc=0.9395\t1930.7 examples/second\n","[Step=9952]\tLoss=0.2288\tacc=0.9397\t1676.3 examples/second\n","[Step=9968]\tLoss=0.2273\tacc=0.9406\t1492.5 examples/second\n","[Step=9984]\tLoss=0.2275\tacc=0.9404\t1558.1 examples/second\n","[Step=10000]\tLoss=0.2271\tacc=0.9409\t1435.4 examples/second\n","[Step=10016]\tLoss=0.2265\tacc=0.9410\t2233.8 examples/second\n","[Step=10032]\tLoss=0.2264\tacc=0.9410\t2619.2 examples/second\n","[Step=10048]\tLoss=0.2278\tacc=0.9404\t2315.3 examples/second\n","[Step=10064]\tLoss=0.2275\tacc=0.9406\t2285.3 examples/second\n","[Step=10080]\tLoss=0.2280\tacc=0.9404\t2364.9 examples/second\n","[Step=10096]\tLoss=0.2277\tacc=0.9403\t2318.0 examples/second\n","[Step=10112]\tLoss=0.2274\tacc=0.9405\t2365.3 examples/second\n","[Step=10128]\tLoss=0.2270\tacc=0.9406\t2514.8 examples/second\n","[Step=10144]\tLoss=0.2276\tacc=0.9403\t3566.5 examples/second\n","[Step=10160]\tLoss=0.2269\tacc=0.9405\t5526.1 examples/second\n","Test Loss=0.3611, Test acc=0.8959\n","\n","Epoch: 26\n","[Step=10176]\tLoss=0.2256\tacc=0.9344\t263.2 examples/second\n","[Step=10192]\tLoss=0.2303\tacc=0.9414\t2246.2 examples/second\n","[Step=10208]\tLoss=0.2258\tacc=0.9422\t2421.7 examples/second\n","[Step=10224]\tLoss=0.2255\tacc=0.9417\t2080.9 examples/second\n","[Step=10240]\tLoss=0.2295\tacc=0.9406\t1405.7 examples/second\n","[Step=10256]\tLoss=0.2345\tacc=0.9385\t1493.8 examples/second\n","[Step=10272]\tLoss=0.2361\tacc=0.9372\t1469.1 examples/second\n","[Step=10288]\tLoss=0.2360\tacc=0.9375\t1517.1 examples/second\n","[Step=10304]\tLoss=0.2323\tacc=0.9391\t1977.5 examples/second\n","[Step=10320]\tLoss=0.2316\tacc=0.9392\t1560.3 examples/second\n","[Step=10336]\tLoss=0.2283\tacc=0.9403\t1460.1 examples/second\n","[Step=10352]\tLoss=0.2298\tacc=0.9399\t1439.7 examples/second\n","[Step=10368]\tLoss=0.2322\tacc=0.9386\t1566.6 examples/second\n","[Step=10384]\tLoss=0.2319\tacc=0.9388\t2273.7 examples/second\n","[Step=10400]\tLoss=0.2316\tacc=0.9389\t2526.7 examples/second\n","[Step=10416]\tLoss=0.2315\tacc=0.9389\t1627.0 examples/second\n","[Step=10432]\tLoss=0.2311\tacc=0.9387\t1546.6 examples/second\n","[Step=10448]\tLoss=0.2321\tacc=0.9382\t1503.0 examples/second\n","[Step=10464]\tLoss=0.2319\tacc=0.9381\t1601.4 examples/second\n","[Step=10480]\tLoss=0.2315\tacc=0.9382\t1551.4 examples/second\n","[Step=10496]\tLoss=0.2305\tacc=0.9385\t2655.8 examples/second\n","[Step=10512]\tLoss=0.2301\tacc=0.9385\t2223.8 examples/second\n","[Step=10528]\tLoss=0.2299\tacc=0.9386\t1735.4 examples/second\n","[Step=10544]\tLoss=0.2306\tacc=0.9386\t5408.0 examples/second\n","Test Loss=0.3605, Test acc=0.8950\n","\n","Epoch: 27\n","[Step=10560]\tLoss=0.2167\tacc=0.9375\t329.5 examples/second\n","[Step=10576]\tLoss=0.2423\tacc=0.9313\t1389.8 examples/second\n","[Step=10592]\tLoss=0.2383\tacc=0.9348\t2467.9 examples/second\n","[Step=10608]\tLoss=0.2348\tacc=0.9355\t2366.5 examples/second\n","[Step=10624]\tLoss=0.2331\tacc=0.9361\t2417.0 examples/second\n","[Step=10640]\tLoss=0.2315\tacc=0.9367\t2351.7 examples/second\n","[Step=10656]\tLoss=0.2362\tacc=0.9358\t2324.8 examples/second\n","[Step=10672]\tLoss=0.2354\tacc=0.9363\t2370.9 examples/second\n","[Step=10688]\tLoss=0.2333\tacc=0.9372\t1560.7 examples/second\n","[Step=10704]\tLoss=0.2341\tacc=0.9370\t1408.7 examples/second\n","[Step=10720]\tLoss=0.2352\tacc=0.9366\t1661.2 examples/second\n","[Step=10736]\tLoss=0.2335\tacc=0.9372\t1502.7 examples/second\n","[Step=10752]\tLoss=0.2327\tacc=0.9377\t1865.3 examples/second\n","[Step=10768]\tLoss=0.2340\tacc=0.9371\t2296.0 examples/second\n","[Step=10784]\tLoss=0.2334\tacc=0.9376\t2479.4 examples/second\n","[Step=10800]\tLoss=0.2332\tacc=0.9379\t2582.2 examples/second\n","[Step=10816]\tLoss=0.2331\tacc=0.9380\t2409.4 examples/second\n","[Step=10832]\tLoss=0.2337\tacc=0.9375\t2311.1 examples/second\n","[Step=10848]\tLoss=0.2328\tacc=0.9378\t2343.2 examples/second\n","[Step=10864]\tLoss=0.2321\tacc=0.9382\t2422.6 examples/second\n","[Step=10880]\tLoss=0.2320\tacc=0.9383\t2473.9 examples/second\n","[Step=10896]\tLoss=0.2316\tacc=0.9384\t2583.5 examples/second\n","[Step=10912]\tLoss=0.2316\tacc=0.9385\t2341.8 examples/second\n","[Step=10928]\tLoss=0.2311\tacc=0.9387\t3993.6 examples/second\n","[Step=10944]\tLoss=0.2312\tacc=0.9386\t5554.3 examples/second\n","Test Loss=0.3629, Test acc=0.8951\n","\n","Epoch: 28\n","[Step=10960]\tLoss=0.2273\tacc=0.9362\t270.9 examples/second\n","[Step=10976]\tLoss=0.2205\tacc=0.9411\t2171.7 examples/second\n","[Step=10992]\tLoss=0.2223\tacc=0.9403\t2222.2 examples/second\n","[Step=11008]\tLoss=0.2221\tacc=0.9404\t2451.1 examples/second\n","[Step=11024]\tLoss=0.2272\tacc=0.9387\t2280.3 examples/second\n","[Step=11040]\tLoss=0.2268\tacc=0.9387\t2374.1 examples/second\n","[Step=11056]\tLoss=0.2249\tacc=0.9397\t2308.7 examples/second\n","[Step=11072]\tLoss=0.2262\tacc=0.9401\t2357.0 examples/second\n","[Step=11088]\tLoss=0.2275\tacc=0.9401\t1949.8 examples/second\n","[Step=11104]\tLoss=0.2274\tacc=0.9402\t1592.2 examples/second\n","[Step=11120]\tLoss=0.2288\tacc=0.9397\t1539.2 examples/second\n","[Step=11136]\tLoss=0.2277\tacc=0.9403\t1583.4 examples/second\n","[Step=11152]\tLoss=0.2267\tacc=0.9407\t1512.5 examples/second\n","[Step=11168]\tLoss=0.2269\tacc=0.9403\t1942.2 examples/second\n","[Step=11184]\tLoss=0.2272\tacc=0.9402\t2455.3 examples/second\n","[Step=11200]\tLoss=0.2273\tacc=0.9402\t2330.0 examples/second\n","[Step=11216]\tLoss=0.2264\tacc=0.9406\t2480.8 examples/second\n","[Step=11232]\tLoss=0.2269\tacc=0.9404\t2280.7 examples/second\n","[Step=11248]\tLoss=0.2270\tacc=0.9404\t2378.7 examples/second\n","[Step=11264]\tLoss=0.2266\tacc=0.9404\t2399.2 examples/second\n","[Step=11280]\tLoss=0.2276\tacc=0.9400\t2399.1 examples/second\n","[Step=11296]\tLoss=0.2283\tacc=0.9399\t2379.9 examples/second\n","[Step=11312]\tLoss=0.2277\tacc=0.9399\t2646.3 examples/second\n","[Step=11328]\tLoss=0.2283\tacc=0.9396\t5645.2 examples/second\n","Test Loss=0.3637, Test acc=0.8947\n","\n","Epoch: 29\n","[Step=11344]\tLoss=0.2436\tacc=0.9297\t272.5 examples/second\n","[Step=11360]\tLoss=0.2330\tacc=0.9356\t1773.7 examples/second\n","[Step=11376]\tLoss=0.2301\tacc=0.9375\t2341.6 examples/second\n","[Step=11392]\tLoss=0.2216\tacc=0.9400\t2341.3 examples/second\n","[Step=11408]\tLoss=0.2245\tacc=0.9384\t2462.6 examples/second\n","[Step=11424]\tLoss=0.2274\tacc=0.9363\t2374.0 examples/second\n","[Step=11440]\tLoss=0.2295\tacc=0.9360\t2324.6 examples/second\n","[Step=11456]\tLoss=0.2289\tacc=0.9371\t2459.1 examples/second\n","[Step=11472]\tLoss=0.2300\tacc=0.9370\t2669.8 examples/second\n","[Step=11488]\tLoss=0.2313\tacc=0.9367\t2180.8 examples/second\n","[Step=11504]\tLoss=0.2303\tacc=0.9369\t1792.8 examples/second\n","[Step=11520]\tLoss=0.2314\tacc=0.9365\t1387.9 examples/second\n","[Step=11536]\tLoss=0.2302\tacc=0.9369\t1532.6 examples/second\n","[Step=11552]\tLoss=0.2289\tacc=0.9377\t1518.2 examples/second\n","[Step=11568]\tLoss=0.2273\tacc=0.9381\t1703.5 examples/second\n","[Step=11584]\tLoss=0.2268\tacc=0.9383\t2465.3 examples/second\n","[Step=11600]\tLoss=0.2259\tacc=0.9385\t2458.8 examples/second\n","[Step=11616]\tLoss=0.2263\tacc=0.9382\t2421.1 examples/second\n","[Step=11632]\tLoss=0.2264\tacc=0.9381\t2382.7 examples/second\n","[Step=11648]\tLoss=0.2261\tacc=0.9380\t2539.9 examples/second\n","[Step=11664]\tLoss=0.2259\tacc=0.9382\t2298.4 examples/second\n","[Step=11680]\tLoss=0.2269\tacc=0.9382\t2335.4 examples/second\n","[Step=11696]\tLoss=0.2270\tacc=0.9381\t2456.9 examples/second\n","[Step=11712]\tLoss=0.2272\tacc=0.9381\t4205.0 examples/second\n","[Step=11728]\tLoss=0.2275\tacc=0.9380\t5605.0 examples/second\n","Test Loss=0.3600, Test acc=0.8947\n"]}]},{"cell_type":"code","source":["# Load the best weight paramters\n","net.load_state_dict(torch.load(\"net_after_pruning.pt\"))\n","test(net)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TTLcipNQoVjX","executionInfo":{"status":"ok","timestamp":1698445125404,"user_tz":240,"elapsed":3957,"user":{"displayName":"Amarnath Shinde","userId":"15881715752542230515"}},"outputId":"57401122-ee89-45ca-9a6c-bf8ea83685c0"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","Test Loss=0.3601, Test accuracy=0.8965\n"]},{"output_type":"execute_result","data":{"text/plain":["0.8965"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["print(\"-----Summary After pruning-----\")\n","summary(net)\n","print(\"-------------------------------\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dXN6ETbmocCl","executionInfo":{"status":"ok","timestamp":1698445135462,"user_tz":240,"elapsed":340,"user":{"displayName":"Amarnath Shinde","userId":"15881715752542230515"}},"outputId":"17eabb10-f0bf-4b36-8768-f9bff73b59c4"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["-----Summary After pruning-----\n","Layer id\tType\t\tParameter\tNon-zero parameter\tSparsity(\\%)\n","1\t\tConvolutional\t864\t\t285\t\t\t0.670139\n","2\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","3\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","4\t\tConvolutional\t9216\t\t2808\t\t\t0.695312\n","5\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","6\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","7\t\tConvolutional\t18432\t\t7769\t\t\t0.578505\n","8\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","9\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","10\t\tConvolutional\t36864\t\t16535\t\t\t0.551459\n","11\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","12\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","13\t\tConvolutional\t73728\t\t32792\t\t\t0.555230\n","14\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","15\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","16\t\tConvolutional\t147456\t\t67257\t\t\t0.543884\n","17\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","18\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","19\t\tConvolutional\t147456\t\t65234\t\t\t0.557604\n","20\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","21\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","22\t\tConvolutional\t294912\t\t125980\t\t\t0.572822\n","23\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","24\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","25\t\tConvolutional\t589824\t\t241013\t\t\t0.591381\n","26\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","27\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","28\t\tConvolutional\t589824\t\t226682\t\t\t0.615679\n","29\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","30\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","31\t\tConvolutional\t589824\t\t199789\t\t\t0.661274\n","32\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","33\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","34\t\tConvolutional\t589824\t\t173869\t\t\t0.705219\n","35\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","36\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","37\t\tConvolutional\t589824\t\t127160\t\t\t0.784410\n","38\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","39\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","40\t\tLinear\t\t65536\t\t28175\t\t\t0.570084\n","41\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","42\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","43\t\tLinear\t\t65536\t\t22524\t\t\t0.656311\n","44\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n","45\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n","46\t\tLinear\t\t2560\t\t389\t\t\t0.848047\n","Total nonzero parameters: 1338261\n","Total parameters: 3811680\n","Total sparsity: 0.648905\n","-------------------------------\n"]}]},{"cell_type":"markdown","metadata":{"id":"SyNext1KyGVG"},"source":["### Quantization"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"gHzyk8a3yGVG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698445241949,"user_tz":240,"elapsed":100715,"user":{"displayName":"Amarnath Shinde","userId":"15881715752542230515"}},"outputId":"712fac55-b8ec-4c5f-af7d-2864942e3d3b"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Complete 1 layers quantization...\n","Complete 2 layers quantization...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Complete 3 layers quantization...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Complete 4 layers quantization...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Complete 5 layers quantization...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Complete 6 layers quantization...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Complete 7 layers quantization...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Complete 8 layers quantization...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Complete 9 layers quantization...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Complete 10 layers quantization...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Complete 11 layers quantization...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Complete 12 layers quantization...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Complete 13 layers quantization...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Complete 14 layers quantization...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Complete 15 layers quantization...\n","Complete 16 layers quantization...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n"]}],"source":["centers = quantize_whole_model(net, bits=4)\n","np.save(\"codebook_vgg16.npy\", centers)"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"26TPGp1ryGVG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698445248480,"user_tz":240,"elapsed":4608,"user":{"displayName":"Amarnath Shinde","userId":"15881715752542230515"}},"outputId":"fd868c95-2b3f-4f2d-e926-f47563edd799"},"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","Test Loss=0.3661, Test accuracy=0.8953\n"]},{"output_type":"execute_result","data":{"text/plain":["0.8953"]},"metadata":{},"execution_count":20}],"source":["test(net)"]},{"cell_type":"markdown","metadata":{"id":"6ShR3qqPyGVG"},"source":["### Huffman Coding"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"JTuku0PWyGVH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698445519389,"user_tz":240,"elapsed":2644,"user":{"displayName":"Amarnath Shinde","userId":"15881715752542230515"}},"outputId":"c6c53eb2-c4de-4316-8be1-9a0b7f763ce9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Original storage for each parameter: 4.0000 bits\n","Average storage for each parameter after Huffman Coding: 2.1632 bits\n","Complete 1 layers for Huffman Coding...\n","Original storage for each parameter: 4.0000 bits\n","Average storage for each parameter after Huffman Coding: 2.0311 bits\n","Complete 2 layers for Huffman Coding...\n","Original storage for each parameter: 4.0000 bits\n","Average storage for each parameter after Huffman Coding: 2.4616 bits\n","Complete 3 layers for Huffman Coding...\n","Original storage for each parameter: 4.0000 bits\n","Average storage for each parameter after Huffman Coding: 2.5796 bits\n","Complete 4 layers for Huffman Coding...\n","Original storage for each parameter: 4.0000 bits\n","Average storage for each parameter after Huffman Coding: 2.5814 bits\n","Complete 5 layers for Huffman Coding...\n","Original storage for each parameter: 4.0000 bits\n","Average storage for each parameter after Huffman Coding: 2.6188 bits\n","Complete 6 layers for Huffman Coding...\n","Original storage for each parameter: 4.0000 bits\n","Average storage for each parameter after Huffman Coding: 2.5452 bits\n","Complete 7 layers for Huffman Coding...\n","Original storage for each parameter: 4.0000 bits\n","Average storage for each parameter after Huffman Coding: 2.4748 bits\n","Complete 8 layers for Huffman Coding...\n","Original storage for each parameter: 4.0000 bits\n","Average storage for each parameter after Huffman Coding: 2.3757 bits\n","Complete 9 layers for Huffman Coding...\n","Original storage for each parameter: 4.0000 bits\n","Average storage for each parameter after Huffman Coding: 2.3092 bits\n","Complete 10 layers for Huffman Coding...\n","Original storage for each parameter: 4.0000 bits\n","Average storage for each parameter after Huffman Coding: 2.1479 bits\n","Complete 11 layers for Huffman Coding...\n","Original storage for each parameter: 4.0000 bits\n","Average storage for each parameter after Huffman Coding: 2.0183 bits\n","Complete 12 layers for Huffman Coding...\n","Original storage for each parameter: 4.0000 bits\n","Average storage for each parameter after Huffman Coding: 1.7272 bits\n","Complete 13 layers for Huffman Coding...\n","Original storage for each parameter: 4.0000 bits\n","Average storage for each parameter after Huffman Coding: 2.4704 bits\n","Complete 14 layers for Huffman Coding...\n","Original storage for each parameter: 4.0000 bits\n","Average storage for each parameter after Huffman Coding: 2.1786 bits\n","Complete 15 layers for Huffman Coding...\n","Original storage for each parameter: 4.0000 bits\n","Average storage for each parameter after Huffman Coding: 1.5797 bits\n","Complete 16 layers for Huffman Coding...\n"]}],"source":["frequency_map, encoding_map = huffman_coding(net, centers)\n","np.save(\"huffman_encoding\", encoding_map)\n","np.save(\"huffman_freq\", frequency_map)"]},{"cell_type":"code","source":["# Initialize variables to store the total weighted storage and total parameters\n","total_weighted_storage = 0.0\n","total_parameters = 0\n","\n","# Iterate through layers in your network\n","for i, (frequencies, encodings) in enumerate(zip(frequency_map, encoding_map), start=1):\n","    layer_name = str(i)  # You may use the layer index as the name\n","\n","    # Calculate the weighted sum for the current layer\n","    weighted_storage_for_layer = sum(frequencies[key] * len(encodings[key]) for key in encodings)\n","\n","    # Add the weighted storage to the total\n","    total_weighted_storage += weighted_storage_for_layer\n","\n","    # Add the total parameters for the current layer to the overall total\n","    total_parameters += sum(frequencies.values())\n","\n","# Calculate the weighted average storage for the entire network\n","weighted_average = total_weighted_storage / total_parameters\n","\n","print(\"Weighted Average Storage: %.2f\" %weighted_average)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z226SWLSBn1W","executionInfo":{"status":"ok","timestamp":1698445521678,"user_tz":240,"elapsed":364,"user":{"displayName":"Amarnath Shinde","userId":"15881715752542230515"}},"outputId":"3fc65088-c1a1-4f65-9664-aef85cabf9b7"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Weighted Average Storage: 2.20\n"]}]},{"cell_type":"code","source":["test(net)"],"metadata":{"id":"fqw43Ft3QRYw","executionInfo":{"status":"ok","timestamp":1698445739293,"user_tz":240,"elapsed":5385,"user":{"displayName":"Amarnath Shinde","userId":"15881715752542230515"}},"outputId":"773ad3fa-5c52-4a86-d348-822b4a30ae7b","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","Test Loss=0.3661, Test accuracy=0.8953\n"]},{"output_type":"execute_result","data":{"text/plain":["0.8953"]},"metadata":{},"execution_count":23}]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"pycharm":{"stem_cell":{"cell_type":"raw","metadata":{"collapsed":false},"source":[]}},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}